<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[织网]]></title>
  <link href="http://zheng-ji.github.com/atom.xml" rel="self"/>
  <link href="http://zheng-ji.github.com/"/>
  <updated>2015-06-12T19:05:23+08:00</updated>
  <id>http://zheng-ji.github.com/</id>
  <author>
    <name><![CDATA[zheng-ji]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[轻巧实时统计用户数]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/06/11/qing-qiao-shi-shi-tong-ji-yong-hu-shu/"/>
    <updated>2015-06-11T23:14:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/06/11/qing-qiao-shi-shi-tong-ji-yong-hu-shu</id>
    <content type="html"><![CDATA[<h2 id="section">背景</h2>

<p>最近在优化一个短地址的统计服务，之前是使用 Cookie 来做统计每天的UV，而且这个需求是近乎实时的，
业务方需要每5分钟就能看到最新统计结果。但有些情况我们是取不到Cookie的，比如服务器对服务器的狂刷访问，那么UV就计算不准确，
是时候要改造方案了。</p>

<p>后来我用 IP+UserAgent 来识别用户，从而统计 UV。好了，接下来你会怎么做这个实时统计呢？</p>

<h2 id="section-1">两个方案的选择</h2>

<ul>
  <li>Plan A：</li>
</ul>

<p>将每天的 <code>IP+UA</code> 存进 Redis 的 Set 集合里，它会自动去重，然后计算该集合里元素的个数得到结果，此方案似乎不错，
但真的好吗？假如每天大概有200W个UV，1个用户标识<code>IP+UA</code>需要大概150个字节，那么大约要耗费300MB的内存。</p>

<p>觉得内存太宝贵，应该有更好的方法，想起了位运算， 于是就有了</p>

<ul>
  <li>Plan B:</li>
</ul>

<p>将 <code>IP+UA</code> 组合成的字符串哈希成一个数值，然后借助 Redis 的 BitSet 数据结构求出<code>UV</code>。以下是伪代码</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class=""><span class="line">conn = redis()
</span><span class="line">index = hash(UA+IP)
</span><span class="line">key = "xxx_2015-05-10"
</span><span class="line">conn.do('SETBIT', key, index, 1) # 将该hash值对应的位赋值为1
</span><span class="line">
</span><span class="line">realtime_uv = conn.do('BITCOUNT', key) # 得到实时的uv</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>根据业务的情况，我的hash桶开了200W个位，大概需要消耗2M的内存，的确节约不少空间，位运算的效率也很快。
于是欣然选择 Plan B</p>

<h2 id="section-2">一些链接</h2>

<ul>
  <li><a href="https://redis.readthedocs.org/en/2.4/set.html">Redis Set</a></li>
  <li><a href="http://redis.io/commands/SETBIT">Redis BitSet</a></li>
  <li><a href="http://blog.nosqlfan.com/html/3501.html">NoSQLFan 一个文章</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搭建Postfix]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/05/17/da-jian-postfix/"/>
    <updated>2015-05-17T00:25:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/05/17/da-jian-postfix</id>
    <content type="html"><![CDATA[<p>我们需要搭建邮件服务，采用Postfix服务, 坑点不少，遂记录。
现在我们决定在 <code>IP：1.2.3.4</code> 的机器上部署<code>Postfix</code>服务，让它可以发邮件</p>

<h3 id="section">安装</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo apt-get install postfix</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-1">配置</h3>

<p>编辑 <code>/etc/postfix/main.cnf</code></p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class=""><span class="line">smtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)
</span><span class="line">biff = no
</span><span class="line">append_dot_mydomain = no
</span><span class="line">readme_directory = no
</span><span class="line">smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
</span><span class="line">smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
</span><span class="line">smtpd_use_tls=yes
</span><span class="line">smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
</span><span class="line">smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache
</span><span class="line">myhostname = mail.zheng-ji.info 邮箱服务器域名
</span><span class="line">alias_maps = hash:/etc/aliases
</span><span class="line">alias_database = hash:/etc/aliases
</span><span class="line">myorigin = $myhostname
</span><span class="line">mydestination = mail.zheng-ji.info, localhost.localdomain, localhost
</span><span class="line">mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128, hash:/etc/postfix/access 可以使用这个邮箱服务的外部地址
</span><span class="line">relay_domains = $mydestination
</span><span class="line">inet_interfaces = all
</span><span class="line">inet_protocols = all</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-2">授权</h3>

<p>为了让邮件能真正到达对方邮箱而不被视为垃圾邮件， 我们需要进行DNS权威认证</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">A 记录指向 1.2.3.4
</span><span class="line">MX 记录也指向 1.2.3.4
</span><span class="line">TXT 记录 v=spf1 ip4:1.2.3.4 ~all</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>以上操作是防止被认为垃圾邮件。</p>

<p>外部需要访问该Postfix 服务发送邮件，需要有access权限,
编辑/etc/postfix/access, 假设 <code>IP:5.6.7.8</code> 的机器想访问该服务</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">5.6.7.8  OK</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>重启并授权生效</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo service postfix restart
</span><span class="line">postmap access</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL Slave Relay log Corrupt 恢复]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/05/10/mysql-slave-relay-log-corrupt-chu-li-he-hui-fu/"/>
    <updated>2015-05-10T20:02:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/05/10/mysql-slave-relay-log-corrupt-chu-li-he-hui-fu</id>
    <content type="html"><![CDATA[<h3 id="section">现象</h3>
<p>周日早晨收到 <code>ganglia</code> 报警, 内容是：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">MySQL_Slave_SQL is 0.00</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>意味着从库同步有问题了。这时候进入从库看看状态</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">show slave status\G;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>看到</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class=""><span class="line">Slave_IO_State: Waiting for master to send event
</span><span class="line">   Master_Host: xxx.xxx.xxx
</span><span class="line">   Master_User: replication
</span><span class="line">   Master_Port: 3306
</span><span class="line">   Connect_Retry: 60
</span><span class="line">   Master_Log_File: mysql-bin.000028
</span><span class="line">   ad_Master_Log_Pos: 982714864
</span><span class="line">   Relay_Log_File: relay-bin.000143
</span><span class="line">   Relay_Master_Log_File: mysql-bin.000028
</span><span class="line">   Slave_IO_Running: Yes
</span><span class="line">   Slave_SQL_Running: No
</span><span class="line">   Replicate_Do_DB:
</span><span class="line"> Replicate_Ignore_DB:
</span><span class="line"> Replicate_Do_Table:
</span><span class="line"> Replicate_Ignore_Table:
</span><span class="line"> Replicate_Wild_Do_Table:
</span><span class="line"> Replicate_Wild_Ignore_Table:
</span><span class="line"> Last_Errno: 1594
</span><span class="line"> Last_Error: Relay log read failure: Could not parse relay log event entry. The possible reasons are: the master's binary log is corrupted (you can check this by running 'mysqlbinlog' on the binary log), the slave's relay log is corrupted (you can check this by running 'mysqlbinlog' on the relay log), a network problem, or a bug in the master's or slave's MySQL code. If you want to check the master's binary log or slave's relay log, you will be able to know their names by issuing 'SHOW SLAVE STATUS' on this slave.
</span><span class="line"> Skip_Counter: 3
</span><span class="line"> Exec_Master_Log_Pos: 974999870
</span><span class="line"> Relay_Log_Space: 399910514
</span><span class="line"> Until_Condition: None</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>是因为从库的 relay log 损坏导致的从库停止执行同步, 后面从 MySQL 的错误日志发现是由几个没经过优化的大查询导致从库内存使用较大，mysqld_safe 被迫重启了。</p>

<h3 id="section-1">解决方法</h3>

<p>找到：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">Relay_Master_Log_File: mysql-bin.000028
</span><span class="line">Exec_Master_Log_Pos: 974999870</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>执行</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">mysql&gt; stop slave ;
</span><span class="line">mysql&gt; change master to master_log_file='mysql-bin.000028',master_log_pos=974999870;
</span><span class="line">mysql&gt; start slave;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这样就重新追上了。以后还是要提高周围同事使用 MySQL 的优化意识啊。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为服务端程序构建docker]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/04/05/yong-bao-docker/"/>
    <updated>2015-04-05T20:24:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/04/05/yong-bao-docker</id>
    <content type="html"><![CDATA[<p>Docker 的优点自从问世就一直被工业界热论。</p>

<p>平时工作中，所部署的大多数<code>Python</code>项目都会用上 <a href="http://wiki.zheng-ji.info/Python/virtualenv-py.html">virtualenv</a>, 
沙箱隔离带来的好处不言而喻。我也希望静态编译的服务，比如 <code>Golang</code> <code>C++</code> 的项目
同样能使用上沙箱环境。得益于<code>Docker</code>，我们仍然可以做到。</p>

<p>这个过程没有想象中的简单，需要一番折腾，我以最近写的 KafkServer 为例，叙述我是怎么构建的，需要读者具备一定的 Docker 基础. 或许这不是最好的方法。</p>

<h3 id="docker-">一览该 Docker 项目</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class=""><span class="line">zj@zheng-ji:~/workspace/gocode/src/kafconsumer/docker$ tree
</span><span class="line">.
</span><span class="line">├── Dockerfile
</span><span class="line">├── kafConsumer
</span><span class="line">│   ├── consumer
</span><span class="line">│   ├── etc
</span><span class="line">│   │   ├── config.yml
</span><span class="line">│   │   └── logger.xml
</span><span class="line">│   └── script
</span><span class="line">│       └── start.sh
</span><span class="line">└── kafConsumer.tar.gz</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>以上的截图，是一个完整的 <code>Docker</code> 项目，包含了：</p>

<ul>
  <li><code>Dockerfile</code>,</li>
  <li><code>kafCounsumer</code>(服务端程序，里面附带的启动脚本，配置程序，以及二进制文件)，</li>
  <li>还有它被压缩而成的 <code>kafConsumer.tar.gz</code></li>
</ul>

<hr />

<h3 id="dockerfile-">Dockerfile 的内容</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">FROM ubuntu:14.04                                                         
</span><span class="line">MAINTAINER zheng-ji &lt;zheng-ji.info&gt;                                     
</span><span class="line">RUN echo Asia/Shanghai &gt; /etc/timezone                   
</span><span class="line">RUN sed -i "s/archive\.ubuntu/mirrors.163/" /etc/apt/sources.list          
</span><span class="line">RUN apt-get update                                                         
</span><span class="line">COPY kafConsumer.tar.gz /                                                  
</span><span class="line">RUN tar xvf kafConsumer.tar.gz                                         
</span><span class="line">VOLUME /data                   
</span><span class="line">WORKDIR /kafConsumer                                                   
</span><span class="line">ENTRYPOINT ["./script/start.sh"]</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><code>Dockerfile</code> 可以理解为<code>makefile</code> 之类的文件，Docker 可以依照文件中的内容，构建镜像.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo docker -t build Server/KafConsumer .</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这样就生成了<code>Tag</code> 为 <code>Server/KafConsumer</code> 的镜像，待会儿我们会使用它</p>

<p>以上 <code>Dockerfile</code> 的具体内容的意义是:</p>

<blockquote>

  <ul>
    <li>第一行：拉取ubuntu 14:04的镜像源</li>
    <li>第二行：维护者</li>
    <li>第三行：调整时区</li>
    <li>第四行：更新源地址</li>
    <li>第五行：更新源</li>
    <li>第六行：复制项目下的压缩包到虚拟机根目录</li>
    <li>第七行：解压</li>
    <li>第八行：项目中使用/data数据卷</li>
    <li>第九行：进入工作目录</li>
    <li>第十行：Docker的入口执行文件是start.sh</li>
  </ul>
</blockquote>

<hr />

<h3 id="section">入口文件的内容</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#!/bin/bash
</span><span class="line">ulimit -a
</span><span class="line">if [ ! -d /data/ad ];  then
</span><span class="line">    mkdir /data/ad
</span><span class="line">fi
</span><span class="line">exec ./consumer -c=etc/config.yml</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这是一个shell的启动文件，因此一定要在开头写明 #!/bin/bash, 使用exec 执行程序</p>

<hr />

<h3 id="section-1">启动镜像</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo docker run -i -t  -v /path/to/data:/data Server/kafConsumer</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>这样就执行了，-v 可以映射你的本地文件到虚拟机的某个数据卷，这样我们就能从外面看到程序产生的文件.</p>

<h3 id="section-2">如果你想关闭或者重启该服务的怎么办</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo docker ps -a
</span><span class="line">
</span><span class="line">找到你的 Docker 容器
</span><span class="line">
</span><span class="line">CONTAINER ID    IMAGE           COMMAND                CREATED        STATUS        PORTS    NAMES
</span><span class="line">5b39d0d5cb85    Server/kafkaconsumer:latest   "./script/start.sh"    3 hours ago    tender_bohr </span></code></pre></td></tr></table></div></figure></notextile></div>

<p>启动或者关闭</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo docker start tender_bohr
</span><span class="line">sudo docker stop tender_bohr</span></code></pre></td></tr></table></div></figure></notextile></div>

<hr />

<h3 id="daocloud--">Daocloud  加速</h3>

<p>功夫墙的原因，国外很多镜像被墙，因此构建镜像很慢，使用 Daocloud 服务可以加速,注册后就有该服务了</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">cat /etc/default/docker
</span><span class="line">DOCKER_OPTS="$DOCKER_OPTS --registry-mirror=http://xxxxxx.m.daocloud.io"</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ansible自动化脚本集]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/04/05/ansiblezi-dong-hua-jiao-ben-ji/"/>
    <updated>2015-04-05T20:22:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/04/05/ansiblezi-dong-hua-jiao-ben-ji</id>
    <content type="html"><![CDATA[<p>管理众多服务器，如果没有自动化的手段，会被累死。</p>

<p>Ansilbe 曾经在 <a href="http://wiki.zheng-ji.info/Sys/ansible.html">WIKI</a> 里有记载，在一定程度上解放了我.</p>

<p>我想记录下平时写的一些<code>Ansible playbook</code>,因此这个文章将会持续更新。</p>

<ul>
  <li>批量修改密码　<code>change-password.yaml</code></li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">---
</span><span class="line">- hosts: my-linode-host
</span><span class="line">  sudo: yes
</span><span class="line">  user: zhengji
</span><span class="line">  tasks:
</span><span class="line">    - name: update password
</span><span class="line">    user: name=guest password=xxxxx</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这里的密码是用md5sum 生成的哈希串，只有这样才能让<code>Ansible</code>识别.</p>

<p>执行</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">ansible-playbook change-password.yaml -K</span></code></pre></td></tr></table></div></figure></notextile></div>

<hr />

<ul>
  <li>批量重启服务</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class=""><span class="line">- hosts: my-linode-host
</span><span class="line">  sudo: yes
</span><span class="line">  tasks:
</span><span class="line">    - name:  supervisorct restart fxserver
</span><span class="line">    shell: supervisorctl restart fxserver
</span><span class="line">    notify:
</span><span class="line">        - Echo hello
</span><span class="line">    #触发调度
</span><span class="line">    handlers:
</span><span class="line">        - name: Echo hello
</span><span class="line">        shell: uptime</span></code></pre></td></tr></table></div></figure></notextile></div>

<hr />

<p>待更新</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[推倒自己]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/03/09/tui-dao-zi-ji/"/>
    <updated>2015-03-09T23:45:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/03/09/tui-dao-zi-ji</id>
    <content type="html"><![CDATA[<p>知乎上面有一个有趣而且严肃的<a href="http://www.zhihu.com/question/24665029/answer/28567915?from=singlemessage&amp;isappinstalled=0">问题</a>, 再次引起我对代码质量的思考。</p>

<p>如知乎所言，代码质量低下如：</p>

<ul>
  <li>文件关系混乱</li>
  <li>注释过期、不明确或者没有</li>
  <li>文档过期、不明确或者没有</li>
  <li>架构乱设计</li>
  <li>过度设计</li>
  <li>不检查用户输入的错误情况</li>
  <li>不检查API或者函数返回的errorcode或者exception</li>
  <li>没有单元测试等自动化测试过程</li>
  <li>编译起来很难</li>
  <li>到处复制代码，公用的部分不整理成内部库</li>
</ul>

<p>垃圾代码存在的客观因素有很多, 包括时间压力，业务留坑，绕开技术难点等。回头发现自己生产出坏代码的时候，除了羞愧，我是带有深深的污点强迫症。就好比一个工匠毕其一生修炼雕塑，却留下令自己鄙视的雕纹, 说道关乎尊严或许也不为过的。</p>

<p>于是我愿意，且习惯性推倒自己的过去，希望用每一个顿悟的今天取代往日无知的自我。</p>

<p>春节的时候学习了 flask, 发现以前写过的一些 Web 项目实现不够优雅，在 flask 对比实现下更是丑陋, 同样的功能，如果换一种方法, 可以感受到简洁，编码组织的顺畅， 更好地部署方式。 遂用了2个晚上推倒了以前写过的一个短地址服务。加上了一些提升用户体验的前端代码, 我想这样或许能给使用这个系统的人带来工作效率的提高吧。</p>

<p>那晚回去的路上, 我是开心的，好比闻到洗好晒干的被子发出的芬芳。我想我应该，也会一直记住这种感觉，并践行着。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[实现一个智能提示框功能]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/02/08/trie-suggestion/"/>
    <updated>2015-02-08T13:40:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/02/08/trie-suggestion</id>
    <content type="html"><![CDATA[<p>耗了3个夜晚出来的东西.</p>

<p>先上图吧：</p>

<p><img src="http://zheng-ji.github.com/images/2015/02/suggest.png" /></p>

<p>是的，就是这样一个类似百度框的联想提升功能，让我纠结了几个晚上，在实现成功的那一瞬间，着实感受到编程之美。很享受为了一个小idea折腾的酣畅淋漓的过程。</p>

<p>起因: 在公司<code>GitLab</code>看到有这人对内部管理系统提出了这个需求，但是一直没有被<code>close</code>, 我觉得应该挺有趣的，好奇心驱动下就开始搞了。</p>

<p>如果仅仅是实现这个需求，应该有很多种</p>

<ul>
  <li>方法一：使用一个hash,将关键字填入key，如果采用此法，数据量大的时候估计堪忧，以一个汉字2个Byte计算的话，1kw个词条，1个词条10个词语的话需要占用大于200M内存。</li>
  <li>方法二：前缀匹配，那么应该怎么选择数据结构,朴素的做法是O(N^N),我们肯定采用复杂度较优的Trie树  O(1)</li>
  <li>方法三：Radix Tree 据说这个是linux cache的一个算法。看了几个小时，真心复杂，不得不佩服内核开发者！</li>
</ul>

<h3 id="trie">讲一讲<code>trie</code>树吧</h3>

<p><img src="http://zheng-ji.github.com/images/2015/02/triestruct.png" /></p>

<ul>
  <li>根节点不包含字符，除根节点外的每一个子节点都包含一个字符。</li>
  <li>根节点到某一节点，路径上经过的字符连接起来，就是该节点对应的字符串。</li>
  <li>每个单词的公共前缀作为一个字符节点保存。</li>
  <li>叶子节点的指针是空的</li>
</ul>

<p>以下是具体的数据结构代码</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">type Node struct {
</span><span class="line">    Link     map[string]*Node  #指针
</span><span class="line">    Key      string                     #每个节点的字符
</span><span class="line">    IsLeaf   bool                       #是否叶子节点 
</span><span class="line">    Weight   float64                    #权重
</span><span class="line">    LongWord string                     #从根节点到该节点的长字符
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>围绕这个数据结构做了
构建树，删除节点，添加节点，获取子节点等</p>

<h3 id="section">知识铺垫：</h3>
<p>在用 Go 语言处理中文字符的时候，需要特别使用 []rune数组，看以下示范代码就知道了,他把中文处理成1个字符表现的编码方式了。正式我们下列处理Trie需要用到的。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class=""><span class="line">package main
</span><span class="line">import "fmt"
</span><span class="line">
</span><span class="line">func main () {
</span><span class="line">    m_str := "编程"
</span><span class="line">    fmt.Println("fmt:", m_str)
</span><span class="line">    m_str_rune := []rune(m_str)
</span><span class="line">    fmt.Println("fmt:", m_str_rune)
</span><span class="line">    m_str_byte := []byte(m_str)
</span><span class="line">    fmt.Println("fmt:", m_str_byte)
</span><span class="line">}
</span><span class="line">
</span><span class="line">$ ./test_rune
</span><span class="line">fmt: 编程
</span><span class="line">fmt: [32534 31243]
</span><span class="line">fmt: [231 188 150 231 168 139]</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-1">测试结果：</h3>
<p>导入100W条词条,搜索的反应是瞬秒，1ms返回响应，在4G的机器上，整个程序占用内存0.3%。</p>

<hr />

<p>每个成熟的互联网产品，背后都是工程师耗费一点一滴思维的结晶构建而成的。对待技术不得不敬畏。</p>

<p><a href="https://github.com/zheng-ji/trietips">代码</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[haproxy - MySQl 的负载均衡]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/02/03/haproxy-plus-mysql/"/>
    <updated>2015-02-03T23:17:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/02/03/haproxy-plus-mysql</id>
    <content type="html"><![CDATA[<p>服务器上每个PHP进程占用一个数据库链接，当有 n 台服务器, 每台服务器用用100 * m 个PHP 进程的时候，数据库的压力是有点小大。</p>

<p>为了解决这个问题, 可以有的选择是：</p>

<ul>
  <li>业内炒的比较火的有，奇虎<a href="https://github.com/Qihoo360/Atlas">Atlas</a>, 淘宝前架构师写的<a href="http://weibo.com/dbatools">OneProxy</a>, 官方的MySQL-Proxy;</li>
  <li>从连接层解决负载均衡的压力，Haproxy 所擅长的事情</li>
</ul>

<p>对于第一个选择，同事做过调研，使用起来不太放心。官方库就无人维护, 于是，最后选择了 Haproxy 来承担数据库的前端代理.链接数下降明显。</p>

<hr />

<h3 id="section">一些关键的配置</h3>

<p>参考<a href="http://www.sysads.co.uk/2014/08/install-haproxy-1-5-6-on-ubuntu-14-04/">连接</a>:</p>

<p>以下是配置内容</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class=""><span class="line">listen mysql-cluster
</span><span class="line">    bind 127.0.0.1:3306  # 连接本地3306 到后端的DB
</span><span class="line">    mode tcp
</span><span class="line">    option mysql-check user haproxy_check # haproxy_check 是该haproxy用户
</span><span class="line">    balance roundrobin
</span><span class="line">    server mysql-1 10.0.0.1:3306 check # 后端DB
</span><span class="line">    server mysql-2 10.0.0.2:3306 check # 后端DB
</span><span class="line">
</span><span class="line">listen 0.0.0.0:8080 # 监控页面
</span><span class="line">    mode http
</span><span class="line">    stats enable
</span><span class="line">    stats uri /
</span><span class="line">    stats realm Strictly\ Private
</span><span class="line">    stats auth A_Username:YourPassword
</span><span class="line">    stats auth Another_User:passwd</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>值得注意的是，我们需要在DB 里面添加用户 haproxy_check,使得它有权限访问这个数据库。一开始我习惯用</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">假设我的内网ip是 192.168.1.5
</span><span class="line">create user 'haproxy_check'@'192.168.1.5' identified by 'xxx';
</span><span class="line">flush privileges;</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>事实上这样连接 haproxy 会报：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">mysql -h127.0.0.1 -uusername -p
</span><span class="line">lost connection to mysql server at 'reading initial communication packet'</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>后来老实按照 digitalocean 的文章修改成</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">INSERT INTO mysql.user (Host,User) values ('192.168.1.5','haproxy_check');
</span><span class="line">flush privileges;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>测试就通过了.好奇怪，在我的理解中很不应该, 明天继续看看为什么会这么奇怪。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搭建elasticsearch与kibana]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/01/31/da-jian-elasticsearchyu-kibana/"/>
    <updated>2015-01-31T12:52:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/01/31/da-jian-elasticsearchyu-kibana</id>
    <content type="html"><![CDATA[<p>ElasticSearch是一个基于Lucene构建的开源，分布式，RESTful搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速,
Kibana 是一个与之配套的 web 界面。</p>

<h3 id="section">安装所需的：</h3>

<ul>
  <li>需要openJDK</li>
  <li>下载并安装 <a href="http://www.elasticsearch.org/overview/elkdownloads/">elasticsearch-1.4.2.deb</a></li>
  <li>下载并安装 <a href="http://www.elasticsearch.org/overview/elkdownloads/">kibana-4.0.0-beta3.tar.gz</a></li>
</ul>

<h3 id="section-1">配置相关</h3>
<p>启动 elasticsearch 方式如下</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo service elasticsearch restart</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>我是使用 <a href="http://wiki.zheng-ji.info/Sys/supervisor.html">supervisor</a> 启动 kibana, </li>
  <li>同时使用 <a href="http://wiki.zheng-ji.info/Sys/monit.html">monit</a> 监控elasticsearch </li>
</ul>

<p>在/etc/defaut/elasticsearch 配置数据文件目录的地址，log 地址，调整内存和堆栈大小，个人认为机器配置的50% 就可以了。</p>

<p>Nginx 配置，使得kibanan可以被外部访问，eleasticsearch 默认监听的是5601：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class=""><span class="line">server {
</span><span class="line">        listen 80;
</span><span class="line">        #auth_basic_user_file /home/ymserver/auth/kibana-user;
</span><span class="line">        error_log /home/ymserver/log/nginx/kibana.err.log;
</span><span class="line">        location / {
</span><span class="line">            proxy_pass http://127.0.0.1:5601$request_uri;
</span><span class="line">            proxy_set_header Host $http_host;
</span><span class="line">            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 
</span><span class="line">            proxy_set_header X-Forwarded-Proto $scheme;
</span><span class="line">        }
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-2">使用过程</h3>
<p>Python 有支持elasticsearch 的库 <a href="https://github.com/elasticsearch/elasticsearch-py">elasticsearch-py</a>，
用其导数据进入elasticsearch,需要指定好索引。</p>

<p>使用restful<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/">查询</a>， 
如下例子：其中’2014-12-18’是索引名，q后面是查询条件，_all表示全部索引</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
</pre></td><td class="code"><pre><code class=""><span class="line">curl -XGET 'http://localhost:9200/2014-12-18/_search/?q=name:861160022835011'
</span><span class="line">curl -XGET 'http://localhost:9200/_all/_search/?q=name:861160022835011'
</span><span class="line">curl -XGET localhost:9200/_search -d '
</span><span class="line">{
</span><span class="line">    "query": {
</span><span class="line">        "bool": {
</span><span class="line">            "must": [
</span><span class="line">            {
</span><span class="line">                "term": {
</span><span class="line">                    "field1": "X"
</span><span class="line">                }
</span><span class="line">            },
</span><span class="line">            {
</span><span class="line">                "term": {
</span><span class="line">                    "field3": "Z"
</span><span class="line">                }
</span><span class="line">            }
</span><span class="line">            ],
</span><span class="line">            "must_not": {
</span><span class="line">                "term": {
</span><span class="line">                    "field2": "Y"
</span><span class="line">                }
</span><span class="line">            }
</span><span class="line">        }
</span><span class="line">    }
</span><span class="line">}'</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-3">性能概述</h3>
<p>导入1个月的日志4.2G，31天的文件，每天一个索引，用了6个小时，elasticsearch用了 6.7G 的空间，在海量数据查询1s内响应。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Percona 小箱里的pt-archiver]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/01/21/percona-toolkitxiao-xiang-li-di-pt-archiver/"/>
    <updated>2015-01-21T23:27:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/01/21/percona-toolkitxiao-xiang-li-di-pt-archiver</id>
    <content type="html"><![CDATA[<p>在管理线上数据库，时常要做一些数据归档操作，在没有了解 <code>Percona toolkit</code> 之前，第一个想到的是在夜深人静的时候使用 <code>MySqlDump</code> 来完成这件事情。但它不是我们的优质选择，理由有：</p>

<ul>
  <li><code>MySqlDump</code> 只能备份在本机，不能直接做远端备份</li>
  <li>导出数据量太大的时候会锁表, 即使它的速度很快，但是在线上服务这是很危险的操作</li>
  <li>它仅仅只能导出，无法做到同时删除(可能不是太有必要)</li>
</ul>

<p>面对上述的场景，<code>Percona Toolkit</code> 让DBA 有了更好地选择，<a href="http://www.percona.com/doc/percona-toolkit/2.1/pt-archiver.html">pt-archiver</a> 应时而生。</p>

<h2 id="pt-archiver-">pt-archiver 介绍：</h2>

<p>根据官方文档的说法，几乎不会对线上的OTLP操作有影响：</p>

<blockquote>
  <p>The goal is a low-impact, forward-only job to nibble old data out of the table without impacting OLTP queries much</p>
</blockquote>

<p>它可以帮助我们将数据归档到文件, 另一个数据库，或者同一个数据库的另一个表, 亦或是用于合并两个表的内容。</p>

<h2 id="section">用法介绍：</h2>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">pt-archiver [OPTION...] --source DSN --where WHERE</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>归档的文件方便使用 load data infile 命令导入数据。另外你还可以用它来执行 delete 操作。这个工具默认的会删除源中的数据，使用的时候请注意。</p>

<p>假如我们将数据库里符合条件的记录归档到文件，并不做删除操作。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">pt-archiver --ask-pass --progress 100000 --no-delete --no-check-charset --source h=localhost,u=root,D=blog,t=comment--file /home/ubuntu/tmp/comment--where 'time &lt; "2013-12-31h"'</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>注意的选项参数：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">--ask-pass        提示要求输密码
</span><span class="line">--progress num    执行num行在界面通知我们
</span><span class="line">--source h,D,t    数据源
</span><span class="line">--no-delete       加上这个参数并不会执行删除操作
</span><span class="line">--dry-run         仅仅将执行语句打印在终端，事实上并不执行。可以用于检测执行过程
</span><span class="line">--where           执行语句，需要用冒号包围起来
</span><span class="line">--limit           批量操作的数量，合理提高这个数值可以加快archive速度</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-1">小总结</h3>

<p>通过开启 mysql 的 <code>general log</code>, 可以发现pt-archiver 执行时，是分批commit 的事务，因此执行效率会慢，在8核16G 内存的生产环境机器备份 1kw 条记录, 耗时150 分钟。 但基本不对服务造成影响，而且可以不用深夜进行, 值得一用。</p>

<p>最近攒了好多好工具和经验，要好好整理搬上来和大家分享才是。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[nginx错误码]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/12/13/nginxcuo-wu-ma/"/>
    <updated>2014-12-13T15:18:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/12/13/nginxcuo-wu-ma</id>
    <content type="html"><![CDATA[<p>在定位线上服务问题的时候，通常会去查看<code>Nginx</code> 的<code>error log</code></p>

<p>那么 error 的定义, 对查找问题就显得很有帮助</p>

<ul>
  <li>upstream prematurely closed connection</li>
</ul>

<blockquote>
  <p>请求uri的时候出现的异常，是由于 upstream 还未返回应答给用户时用户断掉连接造成的，对系统没有影响，可以忽略</p>
</blockquote>

<ul>
  <li>recv() failed (104: Connection reset by peer) </li>
</ul>

<blockquote>
  <p>服务器的并发连接数超过了其承载量，服务器会将其中一些连接Down掉；客户关掉了浏览器，而服务器还在给客户端发送数据;</p>
</blockquote>

<ul>
  <li>(111: Connection refused) while connecting to upstream </li>
</ul>

<blockquote>
  <p>用户在连接时，若遇到后端 upstream 挂掉或者不通，会收到该错误</p>
</blockquote>

<ul>
  <li>(111: Connection refused) while reading response header from upstream </li>
</ul>

<blockquote>
  <p>用户在连接成功后读取数据时，若遇到后端 upstream 挂掉或者不通，会收到该错误</p>
</blockquote>

<ul>
  <li>(111: Connection refused) while sending request to upstream </li>
</ul>

<blockquote>
  <p>Nginx 和 upstream 连接成功后发送数据时，若遇到后端 upstream 挂掉或者不通，会收到该错误</p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while connecting to upstream </li>
</ul>

<blockquote>
  <p>nginx 连接后面的 upstream 时超时</p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while reading upstream </li>
</ul>

<blockquote>
  <p>nginx 读取来自 upstream 的响应时超时 </p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while reading response header from upstream </li>
</ul>

<blockquote>
  <p>nginx 读取来自 upstream 的响应头时超时</p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while reading upstream </li>
</ul>

<blockquote>
  <p>nginx读取来自 upstream 的响应时超时</p>
</blockquote>

<ul>
  <li>(104: Connection reset by peer) while connecting to upstream </li>
</ul>

<blockquote>
  <p>upstream发送了 RST，将连接重置</p>
</blockquote>

<ul>
  <li>upstream sent invalid header while reading response header from upstream </li>
</ul>

<blockquote>
  <p>upstream 发送的响应头无效</p>
</blockquote>

<ul>
  <li>upstream sent no valid HTTP/1.0 header while reading response header from upstream</li>
</ul>

<blockquote>
  <p>upstream 发送的响应头无效</p>
</blockquote>

<ul>
  <li>client intended to send too large body </li>
</ul>

<blockquote>
  <p>用于设置允许接受的客户端请求内容的最大值，默认值是1M，client 发送的 body 超过了设置值</p>
</blockquote>

<ul>
  <li>reopening logs </li>
</ul>

<blockquote>
  <p>用户发送kill  -USR1命令</p>
</blockquote>

<ul>
  <li>gracefully shutting down</li>
</ul>

<blockquote>
  <p>用户发送kill  -WINCH命令</p>
</blockquote>

<ul>
  <li>no live upstreams while connecting to upstream </li>
</ul>

<blockquote>
  <p>upstream 下的 server 全都挂了</p>
</blockquote>

<ul>
  <li>SSL_do_handshake() failed</li>
</ul>

<blockquote>
  <p>SSL握手失败</p>
</blockquote>

<ul>
  <li>ngx_slab_alloc() failed: no memory in SSL session shared cache</li>
</ul>

<blockquote>
  <p>ssl_session_cache大小不够等原因造成</p>
</blockquote>

<ul>
  <li>could not add new SSL session to the session cache while SSL handshaking</li>
</ul>

<blockquote>
  <p>ssl_session_cache 大小不够等原因造成</p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用到的Tcpdump]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/11/29/yong-dao-de-tcpdump/"/>
    <updated>2014-11-29T14:15:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/11/29/yong-dao-de-tcpdump</id>
    <content type="html"><![CDATA[<p>开发中，要定位具体问题，特别是网络问题的时候，多数是要晋出<code>tcpdump</code>，遗憾的是我略懂皮毛，有必要深入一些。
简单说下我常用的 TcpDump的方法</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">tcpdump -i eth0 -Xxn port 80 -s 0 -c 1024 </span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如果仅仅是看manual  多数时候还是会忘记，好记性不如烂笔头，上述的选项是我认为很有用的</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">-i 指定网卡
</span><span class="line">-Xxn X使用Ascii和16进制，n 表示 ip 用数字表示
</span><span class="line">-s 0 表示整包抓取
</span><span class="line">-c 1024 表示包得大小</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如果希望将抓包过程中保留下来，可以在上述命令尾部加上 <code>-w trace.cap</code>
这种格式的文件，文本编辑器是无法理解，需要特殊的软件才能回复，比如 <code>wireshark</code></p>

<p>Tcpdump 中的 flag 有必要提下：</p>

<ul>
  <li>PSH 代表要求发送立即发送缓冲区内的其他对应数据包，无需缓冲区满才发送</li>
  <li>RST 如果RST=1表示连接马上结束，无需等待终止确认手续，发送端已经断线</li>
  <li>SYNC 表示主动连接到对方，建立连接</li>
  <li>FIN 表示传送结束，发送方等待对方响应</li>
</ul>

<p>通过 wireshark 可以再现所谓的三次握手和四次挥手过程。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[慢]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/11/25/tired/"/>
    <updated>2014-11-25T22:20:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/11/25/tired</id>
    <content type="html"><![CDATA[<p>这三个月以来,有30天是凌晨才离开公司的.甚至有好几天都通宵没有睡过,累。</p>

<p>貌似熟悉了凌晨的路灯,和冷冽的寒风。</p>

<p>技术的东西,整理在 EvertNote 里，但想写一些非技术的冲动似乎更强。</p>

<p>生活有很长的路要走, 几乎每天都在发现不足之处,只能让内心更强大, 去接受,去改变, 淡定去拥抱无时不刻的变化, 怀着信念与希望继续走着。</p>

<p>感谢 Everet 一直以来给我的信念, 我们也都在《感谢你让我上场》获得共鸣。我也坚信这位青年会是我们这届的骄傲。</p>

<p>我想让生活慢下来，做一些重要不紧急的事。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx与php-fpm系统参数配置]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/11/02/nginxyu-phpxi-tong-can-shu-pei-zhi/"/>
    <updated>2014-11-02T15:46:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/11/02/nginxyu-phpxi-tong-can-shu-pei-zhi</id>
    <content type="html"><![CDATA[<p>需要为机器调配参数了, 早些时候看过不少文章, 但没经历过始终不深刻, 以下讲的是在 8core 8G，Centos 配置 php-fpm 与 NGINX。</p>

<h3 id="php-fpm-">php-fpm 参数配置</h3>

<p>关于 php-fpm 的配置,Ubuntu系统中 <code>/etc/php5/fpm/pool.d</code>目录下, Centos是<code>/etc/php-fpm.d</code>目录下编辑<code>www.conf</code></p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">listen = /var/run/php5-fpm.sock
</span><span class="line">listen.backlog = -1 (on FreeBSD -1 unlimit)
</span><span class="line">pm = static # 使用静态进程管理
</span><span class="line">pm.max_children = 128
</span><span class="line">request_terminate_timeout = 8s #设置太大，会导致work进程过多，来不及kill掉</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>编辑 <code>/etc/default/php5-fpm</code></p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">ulimit -n 655360</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="nginx-">Nginx 配合参数</h3>

<ul>
  <li>启用irqbalance</li>
</ul>

<p>由于精简系统的服务没有开启irqbalance，irqbalance现在被证实为非常有必要的服务，他的主要功能是可以合理的调配使用各个 CPU 核心，特别是对于目前主流多核心的 CPU，简单的说就是能够把压力均匀的分配到各个 CPU 核心上，对提升性能有很大的帮助。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">shell&gt; yum -y install irqbalance
</span><span class="line">shell&gt; service irqbalance start
</span><span class="line">cat /proc/irqbalance #查看中断的分布</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>为nginx 绑定 cpu</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">worker_rlimit_nofile 300000;
</span><span class="line">worker_processes  8;
</span><span class="line">worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000;</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>连接数调整，</li>
</ul>

<p>nginx发起的连接数，远远超过了 php-fpm 所能处理的数目，导致端口（或socket）频繁被锁，造成堵塞。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class=""><span class="line">vi /etc/sysctl.conf 进行了微调
</span><span class="line">fs.file-max = 6553600
</span><span class="line">
</span><span class="line">vim /etc/security/limits.conf
</span><span class="line">* soft nofile 655360
</span><span class="line">* hard nofile 655360
</span><span class="line">
</span><span class="line">vim /etc/nginx/nginx.conf
</span><span class="line">
</span><span class="line">worker_rlimit_nofile 300000;
</span><span class="line">events {
</span><span class="line">    worker_connections 300000;
</span><span class="line">    use epoll;
</span><span class="line">}
</span><span class="line">http {
</span><span class="line">    keepalive_timeout  0; #关闭keepalive_timeout, 快速释放系统资源
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>从查到的资料看起来 8 核 理論值的最大連線數 = <code>worker_processes * worker_connections / 8</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iptable做NAT转发]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/29/iptablezuo-natzhuan-fa/"/>
    <updated>2014-10-29T22:38:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/29/iptablezuo-natzhuan-fa</id>
    <content type="html"><![CDATA[<p>简单简述下我遇到的问题:</p>

<p>现在局域网有2台机器, 其中一台机器(下文我们称之为ServerA)可以访问外网,有独立IP,而另外一台机器(ServerB)访问不了外网, 需要想办法让 ServerB 也能上网。</p>

<ul>
  <li>ServerA 操作</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">vi /etc/sysctl.conf
</span><span class="line">net.ipv4.ip_forward = 1
</span><span class="line">iptables -t nat -A POSTROUTING -s 10.4.0.0/16 -j MASQUERADE</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>ServerB</p>

<p>编辑 /etc/network/interface
主要是修改gateway 参数，指向 ServerA 的 IP</p>

<p><code>
sudo ifdown eth0; sudo ifup eth0</code> 
ServerB 就可以连接外网了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[唯一索引引发的思考]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/23/mysql-unique-index/"/>
    <updated>2014-10-23T23:18:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/23/mysql-unique-index</id>
    <content type="html"><![CDATA[<p>最近需要改动线上一个有千万条记录的表，涉及到加字段操作，这个表有索引，按照经验，为了加速修改表结构,去掉索引。由于我删除的是Unique Index, 而服务一直在写,程序依赖数据库的唯一索引去重,导致瞬间有重复数据,唯一索引重新加上时会报 <code>Duplicated Key entry Error</code>,</p>

<h3 id="section">回放事件</h3>

<p>我们的数据表，之前是有 <code>UNIQUE INDEX(cuid, aid)</code>, 因为去掉索引，服务持续写入，导致有重复记录，所幸的是，这是一个统计表, 不影响功能，所以需要找出重复的记录</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">select cuid,aid from (
</span><span class="line">    select cuid, aid,count(1) as num
</span><span class="line">    from register_chn 
</span><span class="line">    group by cuid,aid having num &gt; 1
</span><span class="line">) t;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>显示有8条记录,如果手动删除,是很慢且愚蠢的做法,还是用 SQL 执行,镇定之后执行</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">delete from register_chn where (cuid,aid) in (
</span><span class="line">    select cuid,aid from (
</span><span class="line">        select cuid, aid,count(1) as num
</span><span class="line">        from register_chn
</span><span class="line">        group by cuid,aid having num &gt; 1
</span><span class="line">    ) t
</span><span class="line">);</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>影响了８条记录.然后瞬间加上索引.所幸是成功了, 事实上当时的合理操作应该是用事务。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">BEGIN;
</span><span class="line">delete from register_chn where (cuid,aid) in (
</span><span class="line">    select cuid,aid from (
</span><span class="line">        select cuid, aid,count(1) as num
</span><span class="line">        from register_chn
</span><span class="line">        group by cuid,aid having num &gt; 1
</span><span class="line">    ) t
</span><span class="line">);
</span><span class="line">alter table register_chn add unique index(cuid, aid);
</span><span class="line">COMMINT;</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-1">引发的思考</h3>

<p>后来想想, 上述方法虽然解决问题了, 但是有点碰运气成分。如果频繁快速地产生重复记录,也许就没那么好运了,事实上可以执行以下 SQL:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">alter ignore table register_chn add unique index(cuid, aid)；</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如果你以为很简单,那就错了。这个方法在 MySQL 5.0 上使用是没问题的，但是在5.6 之前是有bug的，亲自测试Percona 版本5.5, 的确会失败
官方的解决方法是：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">set session old_alter_table = on;
</span><span class="line">alter ignore table register_chn add unique index(cuid, aid);</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>在有主备的情况,记得执行前设置一下</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">set session sql_log_bin=off. </span></code></pre></td></tr></table></div></figure></notextile></div>
<p>以免备库报错。同样还需要在备库重复一下主库的操作, 这也算是一个不太完美的解决思路。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mysql的一些配置]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/21/mysqlde-na-xie-dong-tai-bian-liang/"/>
    <updated>2014-10-21T23:18:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/21/mysqlde-na-xie-dong-tai-bian-liang</id>
    <content type="html"><![CDATA[<p>我想写一篇长期更新的文章，随着知识积累持续更新的文章，而最近遇到的在涉及的Mysql优化，便是一个可以琢磨的点，变量比较多，并且需要在每次采坑才来的深刻。</p>

<ul>
  <li>tmp_table_size
这个是临时表的大小，与之对应的有 <code>max_heap_size</code> 单位是Byte, <code>tmp_table_size</code> 的大小关乎临时表的大小, 如果设置太小，会导致一些数据直接写入磁盘，比较蛋疼, 看了官方文档，涉及到<code>Dynamic Variable=Yes</code>
那么这种配置便可以直接在终端用语句设置，并生效，不需要重启机器</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">set global tmp_table_size = 512 * 1024 *1024 </span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>max_connections
当<code>show processlist</code> 发现有好多链接的时候，客户端想登陆mysql却失败。可以执行, 看看是不是现有的链接数已经逼近</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">show global variable like '%max%'
</span><span class="line">set global max_connections 1000</span></code></pre></td></tr></table></div></figure></notextile></div>
<ul>
  <li>thread_concurrency</li>
</ul>

<p>同一时间与性的线程设置，如果分配不当，会导致<code>mysql</code>不能充分使用多｀cpu｀, 出现同一时刻只有一个核在工作。
<code>thread_concurrency</code> 应设为<code>CPU</code> 核数的2倍. 比如有一个双核的CPU, 那么<code>thread_concurrency</code>的应该为4; 2个双核的cpu, <code>thread_concurrency</code>的值应为8.</p>

<ul>
  <li>max_allowed_packet
如果需要进行mysqldump这类操作，需要调大</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">set global max_allowed_packet = 2*1024*1024*10</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>skip_slave_start
配置文件中建议加上skip-slave-start，以免在不需要时候slave线程自己开始执行了</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[内存都去哪儿啦]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/13/nei-cun-du-qu-na-er-la/"/>
    <updated>2014-10-13T21:20:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/13/nei-cun-du-qu-na-er-la</id>
    <content type="html"><![CDATA[<h3 id="section">看到的现象</h3>

<p>今天在游戏服务器上使用了top 命令系统使用参数，发现内存16G 的内存用了12G, 如下
<img src="http://zheng-ji.github.com/images/2014/10/top.png" />
但是同时在线人数不多，机器的负载并不是很大，并发也不高, 只有900+。
<img src="http://zheng-ji.github.com/images/2014/10/nestat.png" />
而游戏的服务端代码仅仅使用了7.8G, 我们不禁会问，内存去哪里了？难道是系统自己占用的内存呢，不可能一个8核16G的机器系统自己占用了4G吧?</p>

<p>带着这个疑问，查阅了资料，发现其中并非表明所看到的这么简单. 我们来看以下 使用 <code>free -m</code> 命令
<img src="http://zheng-ji.github.com/images/2014/10/free.png" /></p>

<h3 id="mem--">Mem 参数 解释</h3>
<ul>
  <li>total 内存总数: 15875 , total = used + free</li>
  <li>used 已经使用的内存数: 11900</li>
  <li>free 空闲的内存数: 3975</li>
  <li>shared 当前已经废弃不用，总是0</li>
  <li>buffers: Buffer Cache内存数: 145</li>
  <li>cached: Page Cache内存数: 5561</li>
</ul>

<h3 id="bufferscache">-/+ buffers/cache的解惑</h3>
<ul>
  <li>-buffers/cache 的内存数: 6193 (大致等于第1行的 used - buffers - cached), 反映的是被程序实实在在吃掉的内存，</li>
  <li>+buffers/cache 的内存数: 9682 (大致等于第1行的 free + buffers + cached), 反映的是可以挪用的内存总数</li>
</ul>

<h3 id="cache">两种cache</h3>
<p>因为cpu速度明显快过内存， 为了提高磁盘存取效率, Linux做了一些精心的设计, 采取了两种主要Cache方式, 来做速度的过度, 这些Cache有效缩短了 I/O系统调用(如read,write,getdents)的时间。</p>

<ul>
  <li>Buffer Cache, 针对磁盘块的读写</li>
  <li>Page Cache。针对文件inode的读写。</li>
</ul>

<h3 id="section-1">内存解读的区别</h3>

<p>第1行<code>(mem)的used/free</code>与第2行<code>(-/+ buffers/cache) used/free</code>的区别在于角度的不同:</p>

<ul>
  <li>第一行因为对于OS，buffers/cached 都是属于被使用，所以他的可用内存是3975M,已用内存是11900MB,其中包括,内核（OS）使用 + 应用使用的+ buffers + cached.</li>
  <li>第2行所指的是从应用程序角度来看，对于应用程序来说，buffers/cached 是可用的，是为了提高文件读取的性能而设，当应用程序要用到内存的时候，buffer/cached会很快地被回收。所以从应用程序的角度来说，</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">可用内存=系统free memory + buffers + cached.</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>所以，回到话题开头，虽然内存显示用了12.2G，正在被应用程序使用的是7.9G 还有5.6(cache+buffer) +  4G free 可用.内存原来在这里！</p>

<p>获益匪浅 :) </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[redis热备]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/04/redisre-bei/"/>
    <updated>2014-10-04T22:28:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/04/redisre-bei</id>
    <content type="html"><![CDATA[<p>一转眼到了 10 月份了, 依然很忙, 好多碰到的，解决了的, 没能解决的蛋疼问题, 那些一闪而过的被记载于 evernote里, 信誓旦旦说过要记载下来，终究也还是被时间欺骗。 </p>

<p>在过去的九月份，我们迁移了一批业务到云上。除了 mysql 需要热备处理，redis-server 同样也面临这种问题。相比起来，redis 的热备显得更为简单。</p>

<p>为了可以更好地阐述过程，我们定义了三个角色</p>

<ul>
  <li>A 服务器，原来拥有 redis 数据库的机器 </li>
  <li>B 服务器，未来取代 A 服务器的 redis 数据库服务器</li>
  <li>
    <p>C 服务器，B 服务器的 slave</p>
  </li>
  <li>我们在停止服务的5分钟，在 A 的命令行，执行</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">bgsave</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>使得内存里的数据，完整地保存于 dumps.rdb</p>

<ul>
  <li>
    <p>配置好 B 服务器的redis server, 用上述的 dumps.rdb 取代该服务器的 dumps.rdb , 这里说的配置好，需要指定好 bind-address, 推荐使用内网 IP, 备份目录</p>
  </li>
  <li>
    <p>在 C 服务器的 redis 配置上 更改为:</p>
  </li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">slaveof  IP地址 端口
</span><span class="line">slave-serve-stale-data no
</span><span class="line">slave-read-only yes # 只读</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这样就开始备份了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Percona Server 做主从]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/09/07/percona-server-zuo-zhu-cong/"/>
    <updated>2014-09-07T15:31:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/09/07/percona-server-zuo-zhu-cong</id>
    <content type="html"><![CDATA[<p>数据库做主从，这个过程是需要很耐心的。</p>

<p>数据库做主从目的:</p>

<ul>
  <li>故障恢复， 柔性可用</li>
  <li>也可以做读写分离</li>
</ul>

<p>实验过程中 mysql 用的版本是 Percona Server ，</p>

<p><a href="http://www.percona.com/doc/percona-server/5.5/installation/apt_repo.html">安装过程</a></p>

<p>由于修改默认的数据目录，数据文件不再使用 <code>/var/lib/mysql</code>,数据文件夹被我安放位置是 <code>/data/mysql/data</code> 同时log 目录也放在这里 <code>/data/mysql/log</code> 
注意需要修改目录属主</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo chown mysql:mysql  -R /data/mysql
</span><span class="line">然后执行 sudo mysql_install_db </span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这时候会发生 <code>sudo service mysql stop</code> 失败，原因和方法见此 <a href="http://serverfault.com/questions/32692/cant-start-stop-mysql-service/420222#420222">神贴</a></p>

<p>好了开始进入正题了，备份数据的原理</p>

<ul>
  <li>在主库上把数据更改记录到二进制日志 (Binary Log) </li>
  <li>备库将主库的日志复制到自己的中继日志中</li>
  <li>备库读取中继日志的事件，将其重放到备库的数据之上</li>
</ul>

<p>从其他服务器克隆备库的方法：</p>

<ul>
  <li>使用冷备份：关闭主库，将数据复制到备库，重启主库后，会使用一个新的二进制日志文件，在备库执行 CHANGE MASTER TO 指向这个文件的起始处，缺陷在于关闭主库</li>
  <li>使用热备份，如果仅使用，mysqlhotcopy,或rsync来复制数据</li>
  <li>使用快照或备份：需要知道二进制日志坐标，就可以使用主库的快照和备份来初始化备库，只需要把备份或快照恢复到备库，然后使用 CHANGE MASTER TO 指定二进制日志的坐标.</li>
  <li>用<code>Percona Xtrabackup</code>  个人推荐  <a href="http://www.percona.com/doc/percona-xtrabackup/2.1/howtos/setting_up_replication.html">链接</a></li>
</ul>

<p>======= 实际的步骤如下 ====</p>

<ul>
  <li>备份主库的数据</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">innobackupex --user=root --password=xxx /home/zj/backup</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>在/home/zj/backup目录下就生成了2014-08-21_10-11-4` 目录</p>

<ul>
  <li>复制数据到从库，通过<code>scp</code>将上一步生成的目录放置在从库机器(~/tmp`)将原来的data目录备份, 在从库机器执行</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">mv /data/mysql/data /data/mysql/data_bak
</span><span class="line">mv ~/tmp/2014-08-21_10-11-46  /data/mysql/data
</span><span class="line">chown mysql:mysql -R /data/mysql/data</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>配置主服务器</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">GRANT REPLICATION SLAVE ON *.*  TO 'repl'@'$slaveip' IDENTIFIED BY ''slavepass</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>配置从数据库,并重启</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">server-id=2</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>开始复制</li>
</ul>

<p>需要定位位置</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">cat /data/mysql/data/xtrabackup_binlog_info
</span><span class="line">log-bin.000001     481</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>在从库执行 </li>
</ul>

<p><code>
mysql&gt; CHANGE MASTER TO MASTER_HOST='$masterip',
       MASTER_USER='repl',
       MASTER_PASSWORD='$slavepass',
       MASTER_LOG_FILE='log-bin.000001',
       MASTER_LOG_POS=481;
mysql&gt; START SLAVE;
mysql&gt; SHOW SLAVE STATUS \G
</code></p>

<hr />

<p>系统学习的书籍，感谢杨先生推荐 : )
<a href="http://book.douban.com/subject/23008813/">高性能mysql</a></p>

]]></content>
  </entry>
  
</feed>
