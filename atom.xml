<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[织网]]></title>
  <link href="http://zheng-ji.github.com/atom.xml" rel="self"/>
  <link href="http://zheng-ji.github.com/"/>
  <updated>2015-08-21T16:49:35+08:00</updated>
  <id>http://zheng-ji.github.com/</id>
  <author>
    <name><![CDATA[zheng-ji]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[supervisor监听器]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/08/21/supervisorjian-ting-qi/"/>
    <updated>2015-08-21T16:36:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/08/21/supervisorjian-ting-qi</id>
    <content type="html"><![CDATA[<p>我们服务多是用 supervisor 启动的， 但监控多数是用 <code>monit</code>, 如果我们能通过监测 supervisor 事件变化来做监控，就可以写一套通用的监控程序。</p>

<p>庆幸的是，supervisor 的 <code>eventListener</code> 支持我的设想。</p>

<p>这个监控程序需要用 supervisor 启动，类型不再是<code>program</code>, 而是<code>eventlistener</code>，这里有几个比较耗时的地方需要记录下。</p>

<ul>
  <li>supervisor 有独特的通信协议,需要遵循，否则通讯不会被触发</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class=""><span class="line">def write_stdout(self, s):
</span><span class="line">    sys.stdout.write(s)
</span><span class="line">    sys.stdout.flush()
</span><span class="line">
</span><span class="line">write_stdout('READY\n') //类似开始握手
</span><span class="line">write_stdout('RESULT 2\nOK') //结束通讯</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>需要从标准输入端读取事件,而且他是个阻塞的事件模型</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">while 1:
</span><span class="line">    self.write_stdout('READY\n')
</span><span class="line">    line = sys.stdin.readline()
</span><span class="line">    do_some_thing()
</span><span class="line">    self.write_stdout('RESULT 2\nOK')</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>supervisor 配置文件需要订阅事件</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class=""><span class="line">[eventlistener:alarm]
</span><span class="line">user=zj
</span><span class="line">command=/usr/bin/python /home/ymserver/bin/alarm/main.py
</span><span class="line">events=PROCESS_STATE_EXITED,PROCESS_STATE_STOPPED,PROCESS_STATE_FATAL
</span><span class="line">
</span><span class="line"># 记录控制台输出的日志位置
</span><span class="line">stderr_logfile=/home/zj/log/supervisor/alarm.err.log
</span><span class="line">stdout_logfile=/home/zj/log/supervisor/alarm.output.log</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>弄好 supervisor 配置，以及部署好代码之后，需要重启 supervisor 才会真正的订阅事件。
从此 supervisor 管理的程序一旦有 <code>FATAL</code>,<code>EXIT</code> 等状态就会触发程序，程序中就会触发自定义的报警。</p>

<hr />

<p><a href="https://github.com/zheng-ji/ToyCollection/tree/master/supervisor-listener">代码</a></p>

<p>Happy Hack!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[twemproxy 一个redis代理]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/08/16/twemproxy/"/>
    <updated>2015-08-16T12:11:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/08/16/twemproxy</id>
    <content type="html"><![CDATA[<p>为解决线上 Redis 服务直连出现链接数爆棚而做的调研， 对 Twitter 开源的 twemproxy 做一些记录。 我们之所以放弃官方的 RedisCLuster 是因为不太满意其性能</p>

<ul>
  <li><a href="#第一节">初窥原理</a></li>
  <li><a href="#第二节">安装与配置</a></li>
  <li><a href="#第三节">不支持的操作</a></li>
  <li><a href="#第四节">压力测试</a></li>
  <li><a href="#第五节">摘自极光博客的评论</a></li>
</ul>

<h3 id="第一节">初窥原理</h3>

<ul>
  <li>Twitter 出品的轻量级 Redis，memcached 代理，使用它可以减少缓存服务器的连接数，并且利用它来作分片。</li>
  <li>作是说最差情况下，性能损耗不会多于20%。背后是用了pipeline，redis是支持使用pipeline批处理的。</li>
  <li>twemproxy 与每个 redis 服务器都会建立一个连接，每个连接实现了两个 FIFO 的队列， 通过这两个队列实现对 redis 的 pipeline 访问，将多个客户端的访问合并到一个连接，这样既减少了redis服务器的连接数，又提高了访问性能。</li>
</ul>

<h3 id="第二节">安装与配置</h3>

<ul>
  <li>安装</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class=""><span class="line">apt-get install automake
</span><span class="line">apt-get install libtool
</span><span class="line">git clone git://github.com/twitter/twemproxy.git
</span><span class="line">cd twemproxy
</span><span class="line">autoreconf -fvi
</span><span class="line">./configure
</span><span class="line">make
</span><span class="line">sudo make install</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>默认的可执行文件在 /usr/local/sbin/nutcracker</p>

<ul>
  <li>配置文件 /etc/nutcracker/nutcracker.yml</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class=""><span class="line">alpha:
</span><span class="line">    listen: 127.0.0.1:8877
</span><span class="line">    hash: fnv1a_64
</span><span class="line">    distribution: ketama
</span><span class="line">    auto_eject_hosts: true
</span><span class="line">    redis: true
</span><span class="line">    server_retry_timeout: 30000
</span><span class="line">    server_failure_limit: 3
</span><span class="line">    servers:
</span><span class="line">        - 127.0.0.1:6379:1 master0  #后端的redis-server
</span><span class="line">        - 127.0.0.1:6380:1 master1</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>当 redis 做缓存的使用的时候应该启用 auto_eject_hosts， 如果某个节点失败的时候将该节点删除，虽然丧失了数据的一致性，但作为缓存使用，保证了这个集群的高可用性。当redis做存储的使用时为了保持数据的一致性，应该禁用 auto_eject_hosts,也就是当某个节点失败之后并不删除该节点。</p>

<h3 id="第三节">不支持的操作</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">keys command: keys,migrate,move object,randomkey,rename,renamenx,
</span><span class="line">sort strings command: bitop,mset,msetnx
</span><span class="line">list command: blpop,brpop,brpoplpush
</span><span class="line">scripting command: script exists,script flush,script kill,script load
</span><span class="line">pub/sub command:(全部不支持)psubscribe,publish,punsubscribe,subscribe,unsubscribe</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="第四节">压测</h3>

<p>感谢 redis 提供的 redis-benchmark 工具，用它来做压测挺好的。</p>

<ul>
  <li>n 表示多少个连接</li>
  <li>r 表示多少个 key,</li>
  <li>t 代表命令</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
</pre></td><td class="code"><pre><code class=""><span class="line">zj@zheng-ji.info:~$ redis-benchmark -p 6700 -t smembers,hexists,get,hget,lrange,ltrim,zcard,setex,sadd -n 1000000 -r 100000000
</span><span class="line">
</span><span class="line">====== GET ======
</span><span class="line">1000000 requests completed in 12.95 seconds
</span><span class="line">50 parallel clients
</span><span class="line">3 bytes payload
</span><span class="line">keep alive: 1
</span><span class="line">
</span><span class="line">99.19% &lt;= 1 milliseconds
</span><span class="line">99.93% &lt;= 2 milliseconds
</span><span class="line">100.00% &lt;= 2 milliseconds
</span><span class="line">77220.08 requests per second
</span><span class="line">
</span><span class="line">====== SADD ======
</span><span class="line">1000000 requests completed in 10.74 seconds
</span><span class="line">50 parallel clients
</span><span class="line">3 bytes payload
</span><span class="line">keep alive: 1
</span><span class="line">
</span><span class="line">99.88% &lt;= 1 milliseconds
</span><span class="line">99.95% &lt;= 2 milliseconds
</span><span class="line">99.97% &lt;= 3 milliseconds
</span><span class="line">99.99% &lt;= 4 milliseconds
</span><span class="line">100.00% &lt;= 4 milliseconds
</span><span class="line">93144.56 requests per second</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如作者所言, 性能几乎可以跟直连redis比拟，背后的数据也很均匀,使用twemproxy 观察连接数, 一直都保持在个位数左右。</p>

<h3 id="第五节">摘自极光博客的评论</h3>

<ul>
  <li>前端使用 Twemproxy 做代理，后端的 Redis 数据能基本上根据 key 来进行比较均衡的分布。</li>
  <li>后端一台 Redis 挂掉后，Twemproxy 能够自动摘除。恢复后，Twemproxy 能够自动识别、恢复并重新加入到 Redis 组中重新使用。</li>
  <li>Redis 挂掉后，后端数据是否丢失依据 Redis 本身的策略配置，与 Twemproxy 基本无关。</li>
  <li>如果要新增加一台 Redis，Twemproxy 需要重启才能生效；并且数据不会自动重新 Reblance，需要人工单独写脚本来实现。</li>
  <li>如同时部署多个 Twemproxy，配置文件一致（测试配置为distribution：ketama,modula），则可以从任意一个读取，都可以正确读取 key对应的值。</li>
  <li>多台 Twemproxy 配置一样，客户端分别连接多台 Twemproxy可以在一定条件下提高性能。根据 Server 数量，提高比例在 110-150%之间。</li>
  <li>如原来已经有 2 个节点 Redis，后续有增加 2 个 Redis，则数据分布计算与原来的 Redis 分布无关，现有数据如果需要分布均匀的话，需要人工单独处理。</li>
  <li>如果 Twemproxy 的后端节点数量发生变化，Twemproxy 相同算法的前提下，原来的数据必须重新处理分布，否则会存在找不到key值的情况。</li>
</ul>

<hr />

<p>参考链接</p>

<p><a href="http://blog.jpush.cn/redis-twemproxy-benchmark/">极光推送的博客</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[黑客马拉松]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/08/10/hackthon/"/>
    <updated>2015-08-10T22:24:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/08/10/hackthon</id>
    <content type="html"><![CDATA[<p>周末参加了公司组织的黑客马拉松比赛, </p>

<p>通宵达旦完成了作品是个值得纪念的经历, 我们的产品叫做：<a href="http://pact.im">正义的朋友</a> </p>

<ul>
  <li>我们想要实现的功能是让人在紧急关头用最快速的方式联系到可以帮助你的人，于是我们通过锁屏应用来实现; </li>
  <li>用户在锁屏状态，画一个v手势，会把自己的地理位置发送给设置好的联系人，并持续每十秒发送一次周围的声音给联系人；</li>
  <li>用户在锁屏状态，画一个w手势，会发出专业的求救声</li>
</ul>

<p>这次比赛的感觉, 与当年<code>叫神马</code>团队的似曾相识, 队友非常给力, 大家都很拼, 我喜欢这种感觉, 事实上，在比赛之前，为了工作上的项目我已经几乎透支了, 队友也是一样。 但周六上午我们还是快速进入状态, 被逼的潜力果然不容小觑.  晒一张我们队伍合照.</p>

<p><img src="http://zheng-ji.github.com/images/2015/08/teammate.png" /></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[记录错误登陆的btmp文件]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/07/24/ji-lu-cuo-wu-deng-lu-de-btmpwen-jian/"/>
    <updated>2015-07-24T23:06:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/07/24/ji-lu-cuo-wu-deng-lu-de-btmpwen-jian</id>
    <content type="html"><![CDATA[<p>今天查看了服务器时，发现 <code>/var/log/btmp</code> 日志文件较大。</p>

<p>此文件是记录错误登录的日志， 文件较大意味着有人使用密码字典登录ssh服务，
这个文件是需要用 <code>lastb</code> 命令才可读的。</p>

<p>查看尝试恶意登陆的前十个IP</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo lastb | awk '{ print $3}' | awk '{++S[$NF]} END {for(a in S) print a, S[a]}' | sort -rk2 |head</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如果有有必要封阻IP的话，可以执行：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">iptables -A INPUT -i eth0 -s *.*.*.0/24 -j DROP</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[my.cnf配置依据]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/07/12/my-dot-cnfpei-zhi-yi-ju/"/>
    <updated>2015-07-12T08:35:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/07/12/my-dot-cnfpei-zhi-yi-ju</id>
    <content type="html"><![CDATA[<p>阅读完<a href="http://book.douban.com/subject/6873681/">构建高性能服务器</a> 一书，书中对 <code>MySQL</code> 配置做了讲解，我用烂笔头记录一番，明白它们这么配置的真正意义，形成系统的认知。</p>

<ul>
  <li><a href="#第一节">通用配置</a></li>
  <li><a href="#第二节">innodb_pool_buffer 合理配置</a></li>
  <li><a href="#第三节">文件打开数的合理设置</a></li>
  <li><a href="#第四节">打开表优化设置</a></li>
  <li><a href="#第五节">临时表的设置</a></li>
  <li><a href="#第六节">查看索引命中率</a></li>
</ul>

<h3 id="第一节">通用配置</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">skip-name-resolve:</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>禁止 MySQL 对外连接 DNS 解析 ,这一选项可以消除 MySQL 进行DNS解析时间，开启该选项，所有远程主机连接授权都要使用 IP。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">back_log=512</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>指出在 MySQL 暂停响应之前，有多少个请求被存在堆栈中。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">key_buffer_size</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>指定用于索引的缓冲区大小，增加它可得到更好的索引处理能力，对于内存 4G 左右，可设置为 256MB。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">max_allowed_packet=4M</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>设定在一次网络传输中的最大值。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">thread_stack=256k</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>设置每个线程的堆栈大小，可满足普通操作。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">table_cache=614k</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>高速缓冲区的大小，当mysql访问一个表示，缓冲区还有空间，那么这个表就会被放入缓冲区，一般看峰值时间状态值，open_tables与open_cache，用于判断是否需要增加table_cache，如果open_Table接近table_cache就要增加了。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sort_buffer_size=6M</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>设定查询排序所用的缓冲区大小，是对每个链接而言，如果有100个连接，那么分配的总缓存是100*6=600M。
read_buffer_size,join_buffer_size 都是对单个链接而言。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">thread_cache_size=64</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>设置缓存中的链接线程的最大数量,4GB内存以上的我们会给64或者更大。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">query_cache_size=64M</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如果该值比较小反而会影响效率，可以考虑不用。如果太大，缓存区中碎片会很多。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">tmp_table_size</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>设置内存临时表的最大值，如果超过改制，就会将临时表写入磁盘，范围是1kB-4GB。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">max_connection</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如果超过该值，会出现<code>too many connections</code>。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">max_connect_errors</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>设置每个主机中连接请求异常中断最大次数，如果超过，MySQL禁止host连接请求，直到MySQL重启。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">wait_timeout = 120</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>一个请求的最大链接时间。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">thread_concurrency=8</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>该参数为服务器逻辑CPU * 2。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">skip-networking</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>该选项可以关闭连接方式，如果需要远程连接，不要开启。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">innodb_flush_log_at_trx_commit = 1</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>设置为0就是等到 innodb_log_buffer_size 队列满了之后再统一存储，默认是1，是最安全的设置。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">innodb_thread_concurrency = 8</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>服务器几个CPU就设置多少。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">read_rnd_buffer_Size = 16M</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>设置随机读的使用的缓冲区。</p>

<h3 id="第二节">innodb_buffer_pool 的合理设置</h3>

<p>不要武断的把innodb_buffer pool 配置为内存的50-80% 应具体而定。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">show status like 'innodb_buffer_pool_%'
</span><span class="line">
</span><span class="line">关注的有
</span><span class="line">innodb_buffer_pool_pages_data;
</span><span class="line">innodb_buffer_pool_page_total;
</span><span class="line">innodb_buffer_pool_read_request;
</span><span class="line">innodb_buffer_pool_reads;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>然后看</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">读命中率 (innodb_buffer_pool_read_request - innodb_buffer_pool_reads) / innodb_buffer_pool_read_request;
</span><span class="line">写命中率 innodb_buffer_pool_pages_data /innodb_buffer_pool_page_total;</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>如果读写命中率都能在95%以上就很好了。</p>

<h3 id="第三节">文件打开数的合理设置依据</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">show global status like 'open_files'
</span><span class="line">show variables like 'open_file_limit';</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>比较合适的设置是 Open_files / open_files_limit * 100% &lt;= 75;</p>

<h3 id="第四节">打开表优化设置依据</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">show global status like 'open%tables';
</span><span class="line">show variable like 'table_cache';</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>比较合适</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">open_tables / opende_tables * 100 &gt; 85%;
</span><span class="line">open_Tables / table_Cache* 100% &lt; 95%；</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="第五节">临时表的设置依据</h3>

<p>每次执行语句，关于已经被创造了的临时表的数量，可以这么设置:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">show gloabl status like 'created_tmp5';
</span><span class="line">Created_tmp_disk_table / Created_tmp_tables * 100% &lt;= 25 %</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="第六节">查看索引命中率</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">show global status like 'key_read%';</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>key_cache_miss_rate = key_Read / key_read_request * 100% 小于 1% 是很好的，
意味着1000个请求有一个直接读硬盘。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[轻巧实时统计用户数]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/06/11/qing-qiao-shi-shi-tong-ji-yong-hu-shu/"/>
    <updated>2015-06-11T23:14:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/06/11/qing-qiao-shi-shi-tong-ji-yong-hu-shu</id>
    <content type="html"><![CDATA[<h2 id="section">背景</h2>

<p>最近在优化一个短地址的统计服务，之前是使用 Cookie 来做统计每天的UV，而且这个需求是近乎实时的，
业务方需要每5分钟就能看到最新统计结果。但有些情况我们是取不到Cookie的，比如服务器对服务器的狂刷访问，那么UV就计算不准确，
是时候要改造方案了。</p>

<p>后来我用 IP+UserAgent 来识别用户，从而统计 UV。好了，接下来你会怎么做这个实时统计呢？</p>

<h2 id="section-1">两个方案的选择</h2>

<ul>
  <li>Plan A：</li>
</ul>

<p>将每天的 <code>IP+UA</code> 存进 Redis 的 Set 集合里，它会自动去重，然后计算该集合里元素的个数得到结果，此方案似乎不错，
但真的好吗？假如每天大概有200W个UV，1个用户标识<code>IP+UA</code>需要大概150个字节，那么大约要耗费300MB的内存。</p>

<p>觉得内存太宝贵，应该有更好的方法，想起了位运算， 于是就有了</p>

<ul>
  <li>Plan B:</li>
</ul>

<p>将 <code>IP+UA</code> 组合成的字符串哈希成一个数值，然后借助 Redis 的 BitSet 数据结构求出<code>UV</code>。以下是伪代码</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class=""><span class="line">conn = redis()
</span><span class="line">index = hash(UA+IP)
</span><span class="line">key = "xxx_2015-05-10"
</span><span class="line">conn.do('SETBIT', key, index, 1) # 将该hash值对应的位赋值为1
</span><span class="line">
</span><span class="line">realtime_uv = conn.do('BITCOUNT', key) # 得到实时的uv</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>根据业务的情况，我的hash桶开了200W个位，大概需要消耗2M的内存，的确节约不少空间，位运算的效率也很快。
于是欣然选择 Plan B</p>

<h2 id="section-2">一些链接</h2>

<ul>
  <li><a href="https://redis.readthedocs.org/en/2.4/set.html">Redis Set</a></li>
  <li><a href="http://redis.io/commands/SETBIT">Redis BitSet</a></li>
  <li><a href="http://blog.nosqlfan.com/html/3501.html">NoSQLFan 一个文章</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搭建Postfix]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/05/17/da-jian-postfix/"/>
    <updated>2015-05-17T00:25:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/05/17/da-jian-postfix</id>
    <content type="html"><![CDATA[<p>我们需要搭建邮件服务，采用Postfix服务, 坑点不少，遂记录。
现在我们决定在 <code>IP：1.2.3.4</code> 的机器上部署<code>Postfix</code>服务，让它可以发邮件</p>

<h3 id="section">安装</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo apt-get install postfix</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-1">配置</h3>

<p>编辑 <code>/etc/postfix/main.cnf</code></p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class=""><span class="line">smtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)
</span><span class="line">biff = no
</span><span class="line">append_dot_mydomain = no
</span><span class="line">readme_directory = no
</span><span class="line">smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
</span><span class="line">smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
</span><span class="line">smtpd_use_tls=yes
</span><span class="line">smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache
</span><span class="line">smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache
</span><span class="line">myhostname = mail.zheng-ji.info 邮箱服务器域名
</span><span class="line">alias_maps = hash:/etc/aliases
</span><span class="line">alias_database = hash:/etc/aliases
</span><span class="line">myorigin = $myhostname
</span><span class="line">mydestination = mail.zheng-ji.info, localhost.localdomain, localhost
</span><span class="line">mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128, hash:/etc/postfix/access 可以使用这个邮箱服务的外部地址
</span><span class="line">relay_domains = $mydestination
</span><span class="line">inet_interfaces = all
</span><span class="line">inet_protocols = all</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-2">授权</h3>

<p>为了让邮件能真正到达对方邮箱而不被视为垃圾邮件， 我们需要进行DNS权威认证</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">A 记录指向 1.2.3.4
</span><span class="line">MX 记录也指向 1.2.3.4
</span><span class="line">TXT 记录 v=spf1 ip4:1.2.3.4 ~all</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>以上操作是防止被认为垃圾邮件。</p>

<p>外部需要访问该Postfix 服务发送邮件，需要有access权限,
编辑/etc/postfix/access, 假设 <code>IP:5.6.7.8</code> 的机器想访问该服务</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">5.6.7.8  OK</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>重启并授权生效</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo service postfix restart
</span><span class="line">postmap access</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL Slave Relay log Corrupt 恢复]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/05/10/mysql-slave-relay-log-corrupt-chu-li-he-hui-fu/"/>
    <updated>2015-05-10T20:02:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/05/10/mysql-slave-relay-log-corrupt-chu-li-he-hui-fu</id>
    <content type="html"><![CDATA[<h3 id="section">现象</h3>
<p>周日早晨收到 <code>ganglia</code> 报警, 内容是：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">MySQL_Slave_SQL is 0.00</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>意味着从库同步有问题了。这时候进入从库看看状态</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">show slave status\G;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>看到</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class=""><span class="line">Slave_IO_State: Waiting for master to send event
</span><span class="line">   Master_Host: xxx.xxx.xxx
</span><span class="line">   Master_User: replication
</span><span class="line">   Master_Port: 3306
</span><span class="line">   Connect_Retry: 60
</span><span class="line">   Master_Log_File: mysql-bin.000028
</span><span class="line">   ad_Master_Log_Pos: 982714864
</span><span class="line">   Relay_Log_File: relay-bin.000143
</span><span class="line">   Relay_Master_Log_File: mysql-bin.000028
</span><span class="line">   Slave_IO_Running: Yes
</span><span class="line">   Slave_SQL_Running: No
</span><span class="line">   Replicate_Do_DB:
</span><span class="line"> Replicate_Ignore_DB:
</span><span class="line"> Replicate_Do_Table:
</span><span class="line"> Replicate_Ignore_Table:
</span><span class="line"> Replicate_Wild_Do_Table:
</span><span class="line"> Replicate_Wild_Ignore_Table:
</span><span class="line"> Last_Errno: 1594
</span><span class="line"> Last_Error: Relay log read failure: Could not parse relay log event entry. The possible reasons are: the master's binary log is corrupted (you can check this by running 'mysqlbinlog' on the binary log), the slave's relay log is corrupted (you can check this by running 'mysqlbinlog' on the relay log), a network problem, or a bug in the master's or slave's MySQL code. If you want to check the master's binary log or slave's relay log, you will be able to know their names by issuing 'SHOW SLAVE STATUS' on this slave.
</span><span class="line"> Skip_Counter: 3
</span><span class="line"> Exec_Master_Log_Pos: 974999870
</span><span class="line"> Relay_Log_Space: 399910514
</span><span class="line"> Until_Condition: None</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>是因为从库的 relay log 损坏导致的从库停止执行同步, 后面从 MySQL 的错误日志发现是由几个没经过优化的大查询导致从库内存使用较大，mysqld_safe 被迫重启了。</p>

<h3 id="section-1">解决方法</h3>

<p>找到：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">Relay_Master_Log_File: mysql-bin.000028
</span><span class="line">Exec_Master_Log_Pos: 974999870</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>执行</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">mysql&gt; stop slave ;
</span><span class="line">mysql&gt; change master to master_log_file='mysql-bin.000028',master_log_pos=974999870;
</span><span class="line">mysql&gt; start slave;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这样就重新追上了。以后还是要提高周围同事使用 MySQL 的优化意识啊。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为服务端程序构建docker]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/04/05/yong-bao-docker/"/>
    <updated>2015-04-05T20:24:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/04/05/yong-bao-docker</id>
    <content type="html"><![CDATA[<p>Docker 的优点自从问世就一直被工业界热论。</p>

<p>平时工作中，所部署的大多数<code>Python</code>项目都会用上 <a href="http://wiki.zheng-ji.info/Python/virtualenv-py.html">virtualenv</a>, 
沙箱隔离带来的好处不言而喻。我也希望静态编译的服务，比如 <code>Golang</code> <code>C++</code> 的项目
同样能使用上沙箱环境。得益于<code>Docker</code>，我们仍然可以做到。</p>

<p>这个过程没有想象中的简单，需要一番折腾，我以最近写的 KafkServer 为例，叙述我是怎么构建的，需要读者具备一定的 Docker 基础. 或许这不是最好的方法。</p>

<h3 id="docker-">一览该 Docker 项目</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class=""><span class="line">zj@zheng-ji:~/workspace/gocode/src/kafconsumer/docker$ tree
</span><span class="line">.
</span><span class="line">├── Dockerfile
</span><span class="line">├── kafConsumer
</span><span class="line">│   ├── consumer
</span><span class="line">│   ├── etc
</span><span class="line">│   │   ├── config.yml
</span><span class="line">│   │   └── logger.xml
</span><span class="line">│   └── script
</span><span class="line">│       └── start.sh
</span><span class="line">└── kafConsumer.tar.gz</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>以上的截图，是一个完整的 <code>Docker</code> 项目，包含了：</p>

<ul>
  <li><code>Dockerfile</code>,</li>
  <li><code>kafCounsumer</code>(服务端程序，里面附带的启动脚本，配置程序，以及二进制文件)，</li>
  <li>还有它被压缩而成的 <code>kafConsumer.tar.gz</code></li>
</ul>

<hr />

<h3 id="dockerfile-">Dockerfile 的内容</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">FROM ubuntu:14.04                                                         
</span><span class="line">MAINTAINER zheng-ji &lt;zheng-ji.info&gt;                                     
</span><span class="line">RUN echo Asia/Shanghai &gt; /etc/timezone                   
</span><span class="line">RUN sed -i "s/archive\.ubuntu/mirrors.163/" /etc/apt/sources.list          
</span><span class="line">RUN apt-get update                                                         
</span><span class="line">COPY kafConsumer.tar.gz /                                                  
</span><span class="line">RUN tar xvf kafConsumer.tar.gz                                         
</span><span class="line">VOLUME /data                   
</span><span class="line">WORKDIR /kafConsumer                                                   
</span><span class="line">ENTRYPOINT ["./script/start.sh"]</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><code>Dockerfile</code> 可以理解为<code>makefile</code> 之类的文件，Docker 可以依照文件中的内容，构建镜像.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo docker -t build Server/KafConsumer .</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这样就生成了<code>Tag</code> 为 <code>Server/KafConsumer</code> 的镜像，待会儿我们会使用它</p>

<p>以上 <code>Dockerfile</code> 的具体内容的意义是:</p>

<blockquote>

  <ul>
    <li>第一行：拉取ubuntu 14:04的镜像源</li>
    <li>第二行：维护者</li>
    <li>第三行：调整时区</li>
    <li>第四行：更新源地址</li>
    <li>第五行：更新源</li>
    <li>第六行：复制项目下的压缩包到虚拟机根目录</li>
    <li>第七行：解压</li>
    <li>第八行：项目中使用/data数据卷</li>
    <li>第九行：进入工作目录</li>
    <li>第十行：Docker的入口执行文件是start.sh</li>
  </ul>
</blockquote>

<hr />

<h3 id="section">入口文件的内容</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#!/bin/bash
</span><span class="line">ulimit -a
</span><span class="line">if [ ! -d /data/ad ];  then
</span><span class="line">    mkdir /data/ad
</span><span class="line">fi
</span><span class="line">exec ./consumer -c=etc/config.yml</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这是一个shell的启动文件，因此一定要在开头写明 #!/bin/bash, 使用exec 执行程序</p>

<hr />

<h3 id="section-1">启动镜像</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo docker run -i -t  -v /path/to/data:/data Server/kafConsumer</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>这样就执行了，-v 可以映射你的本地文件到虚拟机的某个数据卷，这样我们就能从外面看到程序产生的文件.</p>

<h3 id="section-2">如果你想关闭或者重启该服务的怎么办</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo docker ps -a
</span><span class="line">
</span><span class="line">找到你的 Docker 容器
</span><span class="line">
</span><span class="line">CONTAINER ID    IMAGE           COMMAND                CREATED        STATUS        PORTS    NAMES
</span><span class="line">5b39d0d5cb85    Server/kafkaconsumer:latest   "./script/start.sh"    3 hours ago    tender_bohr </span></code></pre></td></tr></table></div></figure></notextile></div>

<p>启动或者关闭</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo docker start tender_bohr
</span><span class="line">sudo docker stop tender_bohr</span></code></pre></td></tr></table></div></figure></notextile></div>

<hr />

<h3 id="daocloud--">Daocloud  加速</h3>

<p>功夫墙的原因，国外很多镜像被墙，因此构建镜像很慢，使用 Daocloud 服务可以加速,注册后就有该服务了</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">cat /etc/default/docker
</span><span class="line">DOCKER_OPTS="$DOCKER_OPTS --registry-mirror=http://xxxxxx.m.daocloud.io"</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[推倒自己]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/03/09/tui-dao-zi-ji/"/>
    <updated>2015-03-09T23:45:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/03/09/tui-dao-zi-ji</id>
    <content type="html"><![CDATA[<p>知乎上面有一个有趣而且严肃的<a href="http://www.zhihu.com/question/24665029/answer/28567915?from=singlemessage&amp;isappinstalled=0">问题</a>, 再次引起我对代码质量的思考。</p>

<p>如知乎所言，代码质量低下如：</p>

<ul>
  <li>文件关系混乱</li>
  <li>注释过期、不明确或者没有</li>
  <li>文档过期、不明确或者没有</li>
  <li>架构乱设计</li>
  <li>过度设计</li>
  <li>不检查用户输入的错误情况</li>
  <li>不检查API或者函数返回的errorcode或者exception</li>
  <li>没有单元测试等自动化测试过程</li>
  <li>编译起来很难</li>
  <li>到处复制代码，公用的部分不整理成内部库</li>
</ul>

<p>垃圾代码存在的客观因素有很多, 包括时间压力，业务留坑，绕开技术难点等。回头发现自己生产出坏代码的时候，除了羞愧，我是带有深深的污点强迫症。就好比一个工匠毕其一生修炼雕塑，却留下令自己鄙视的雕纹, 说道关乎尊严或许也不为过的。</p>

<p>于是我愿意，且习惯性推倒自己的过去，希望用每一个顿悟的今天取代往日无知的自我。</p>

<p>春节的时候学习了 flask, 发现以前写过的一些 Web 项目实现不够优雅，在 flask 对比实现下更是丑陋, 同样的功能，如果换一种方法, 可以感受到简洁，编码组织的顺畅， 更好地部署方式。 遂用了2个晚上推倒了以前写过的一个短地址服务。加上了一些提升用户体验的前端代码, 我想这样或许能给使用这个系统的人带来工作效率的提高吧。</p>

<p>那晚回去的路上, 我是开心的，好比闻到洗好晒干的被子发出的芬芳。我想我应该，也会一直记住这种感觉，并践行着。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[实现一个智能提示框功能]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/02/08/trie-suggestion/"/>
    <updated>2015-02-08T13:40:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/02/08/trie-suggestion</id>
    <content type="html"><![CDATA[<p>耗了3个夜晚出来的东西.</p>

<p>先上图吧：</p>

<p><img src="http://zheng-ji.github.com/images/2015/02/suggest.png" /></p>

<p>是的，就是这样一个类似百度框的联想提升功能，让我纠结了几个晚上，在实现成功的那一瞬间，着实感受到编程之美。很享受为了一个小idea折腾的酣畅淋漓的过程。</p>

<p>起因: 在公司<code>GitLab</code>看到有这人对内部管理系统提出了这个需求，但是一直没有被<code>close</code>, 我觉得应该挺有趣的，好奇心驱动下就开始搞了。</p>

<p>如果仅仅是实现这个需求，应该有很多种</p>

<ul>
  <li>方法一：使用一个hash,将关键字填入key，如果采用此法，数据量大的时候估计堪忧，以一个汉字2个Byte计算的话，1kw个词条，1个词条10个词语的话需要占用大于200M内存。</li>
  <li>方法二：前缀匹配，那么应该怎么选择数据结构,朴素的做法是O(N^N),我们肯定采用复杂度较优的Trie树  O(1)</li>
  <li>方法三：Radix Tree 据说这个是linux cache的一个算法。看了几个小时，真心复杂，不得不佩服内核开发者！</li>
</ul>

<h3 id="trie">讲一讲<code>trie</code>树吧</h3>

<p><img src="http://zheng-ji.github.com/images/2015/02/triestruct.png" /></p>

<ul>
  <li>根节点不包含字符，除根节点外的每一个子节点都包含一个字符。</li>
  <li>根节点到某一节点，路径上经过的字符连接起来，就是该节点对应的字符串。</li>
  <li>每个单词的公共前缀作为一个字符节点保存。</li>
  <li>叶子节点的指针是空的</li>
</ul>

<p>以下是具体的数据结构代码</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">type Node struct {
</span><span class="line">    Link     map[string]*Node  #指针
</span><span class="line">    Key      string                     #每个节点的字符
</span><span class="line">    IsLeaf   bool                       #是否叶子节点 
</span><span class="line">    Weight   float64                    #权重
</span><span class="line">    LongWord string                     #从根节点到该节点的长字符
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>围绕这个数据结构做了
构建树，删除节点，添加节点，获取子节点等</p>

<h3 id="section">知识铺垫：</h3>
<p>在用 Go 语言处理中文字符的时候，需要特别使用 []rune数组，看以下示范代码就知道了,他把中文处理成1个字符表现的编码方式了。正式我们下列处理Trie需要用到的。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class=""><span class="line">package main
</span><span class="line">import "fmt"
</span><span class="line">
</span><span class="line">func main () {
</span><span class="line">    m_str := "编程"
</span><span class="line">    fmt.Println("fmt:", m_str)
</span><span class="line">    m_str_rune := []rune(m_str)
</span><span class="line">    fmt.Println("fmt:", m_str_rune)
</span><span class="line">    m_str_byte := []byte(m_str)
</span><span class="line">    fmt.Println("fmt:", m_str_byte)
</span><span class="line">}
</span><span class="line">
</span><span class="line">$ ./test_rune
</span><span class="line">fmt: 编程
</span><span class="line">fmt: [32534 31243]
</span><span class="line">fmt: [231 188 150 231 168 139]</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-1">测试结果：</h3>
<p>导入100W条词条,搜索的反应是瞬秒，1ms返回响应，在4G的机器上，整个程序占用内存0.3%。</p>

<hr />

<p>每个成熟的互联网产品，背后都是工程师耗费一点一滴思维的结晶构建而成的。对待技术不得不敬畏。</p>

<p><a href="https://github.com/zheng-ji/trietips">代码</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[haproxy - MySQl 的负载均衡]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/02/03/haproxy-plus-mysql/"/>
    <updated>2015-02-03T23:17:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/02/03/haproxy-plus-mysql</id>
    <content type="html"><![CDATA[<p>服务器上每个PHP进程占用一个数据库链接，当有 n 台服务器, 每台服务器用用100 * m 个PHP 进程的时候，数据库的压力是有点小大。</p>

<p>为了解决这个问题, 可以有的选择是：</p>

<ul>
  <li>业内炒的比较火的有，奇虎<a href="https://github.com/Qihoo360/Atlas">Atlas</a>, 淘宝前架构师写的<a href="http://weibo.com/dbatools">OneProxy</a>, 官方的MySQL-Proxy;</li>
  <li>从连接层解决负载均衡的压力，Haproxy 所擅长的事情</li>
</ul>

<p>对于第一个选择，同事做过调研，使用起来不太放心。官方库就无人维护, 于是，最后选择了 Haproxy 来承担数据库的前端代理.链接数下降明显。</p>

<hr />

<h3 id="section">一些关键的配置</h3>

<p>参考<a href="http://www.sysads.co.uk/2014/08/install-haproxy-1-5-6-on-ubuntu-14-04/">连接</a>:</p>

<p>以下是配置内容</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class=""><span class="line">listen mysql-cluster
</span><span class="line">    bind 127.0.0.1:3306  # 连接本地3306 到后端的DB
</span><span class="line">    mode tcp
</span><span class="line">    option mysql-check user haproxy_check # haproxy_check 是该haproxy用户
</span><span class="line">    balance roundrobin
</span><span class="line">    server mysql-1 10.0.0.1:3306 check # 后端DB
</span><span class="line">    server mysql-2 10.0.0.2:3306 check # 后端DB
</span><span class="line">
</span><span class="line">listen 0.0.0.0:8080 # 监控页面
</span><span class="line">    mode http
</span><span class="line">    stats enable
</span><span class="line">    stats uri /
</span><span class="line">    stats realm Strictly\ Private
</span><span class="line">    stats auth A_Username:YourPassword
</span><span class="line">    stats auth Another_User:passwd</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>值得注意的是，我们需要在DB 里面添加用户 haproxy_check,使得它有权限访问这个数据库。一开始我习惯用</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">假设我的内网ip是 192.168.1.5
</span><span class="line">create user 'haproxy_check'@'192.168.1.5' identified by 'xxx';
</span><span class="line">flush privileges;</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>事实上这样连接 haproxy 会报：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">mysql -h127.0.0.1 -uusername -p
</span><span class="line">lost connection to mysql server at 'reading initial communication packet'</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>后来老实按照 digitalocean 的文章修改成</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">INSERT INTO mysql.user (Host,User) values ('192.168.1.5','haproxy_check');
</span><span class="line">flush privileges;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>测试就通过了.好奇怪，在我的理解中很不应该, 明天继续看看为什么会这么奇怪。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搭建elasticsearch与kibana]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/01/31/da-jian-elasticsearchyu-kibana/"/>
    <updated>2015-01-31T12:52:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/01/31/da-jian-elasticsearchyu-kibana</id>
    <content type="html"><![CDATA[<p>ElasticSearch是一个基于Lucene构建的开源，分布式，RESTful搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速,
Kibana 是一个与之配套的 web 界面。</p>

<h3 id="section">安装所需的：</h3>

<ul>
  <li>需要openJDK</li>
  <li>下载并安装 <a href="http://www.elasticsearch.org/overview/elkdownloads/">elasticsearch-1.4.2.deb</a></li>
  <li>下载并安装 <a href="http://www.elasticsearch.org/overview/elkdownloads/">kibana-4.0.0-beta3.tar.gz</a></li>
</ul>

<h3 id="section-1">配置相关</h3>
<p>启动 elasticsearch 方式如下</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo service elasticsearch restart</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>我是使用 <a href="http://wiki.zheng-ji.info/Sys/supervisor.html">supervisor</a> 启动 kibana, </li>
  <li>同时使用 <a href="http://wiki.zheng-ji.info/Sys/monit.html">monit</a> 监控elasticsearch </li>
</ul>

<p>在/etc/defaut/elasticsearch 配置数据文件目录的地址，log 地址，调整内存和堆栈大小，个人认为机器配置的50% 就可以了。</p>

<p>Nginx 配置，使得kibanan可以被外部访问，eleasticsearch 默认监听的是5601：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class=""><span class="line">server {
</span><span class="line">        listen 80;
</span><span class="line">        #auth_basic_user_file /home/ymserver/auth/kibana-user;
</span><span class="line">        error_log /home/ymserver/log/nginx/kibana.err.log;
</span><span class="line">        location / {
</span><span class="line">            proxy_pass http://127.0.0.1:5601$request_uri;
</span><span class="line">            proxy_set_header Host $http_host;
</span><span class="line">            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 
</span><span class="line">            proxy_set_header X-Forwarded-Proto $scheme;
</span><span class="line">        }
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-2">使用过程</h3>
<p>Python 有支持elasticsearch 的库 <a href="https://github.com/elasticsearch/elasticsearch-py">elasticsearch-py</a>，
用其导数据进入elasticsearch,需要指定好索引。</p>

<p>使用restful<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/">查询</a>， 
如下例子：其中’2014-12-18’是索引名，q后面是查询条件，_all表示全部索引</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
</pre></td><td class="code"><pre><code class=""><span class="line">curl -XGET 'http://localhost:9200/2014-12-18/_search/?q=name:861160022835011'
</span><span class="line">curl -XGET 'http://localhost:9200/_all/_search/?q=name:861160022835011'
</span><span class="line">curl -XGET localhost:9200/_search -d '
</span><span class="line">{
</span><span class="line">    "query": {
</span><span class="line">        "bool": {
</span><span class="line">            "must": [
</span><span class="line">            {
</span><span class="line">                "term": {
</span><span class="line">                    "field1": "X"
</span><span class="line">                }
</span><span class="line">            },
</span><span class="line">            {
</span><span class="line">                "term": {
</span><span class="line">                    "field3": "Z"
</span><span class="line">                }
</span><span class="line">            }
</span><span class="line">            ],
</span><span class="line">            "must_not": {
</span><span class="line">                "term": {
</span><span class="line">                    "field2": "Y"
</span><span class="line">                }
</span><span class="line">            }
</span><span class="line">        }
</span><span class="line">    }
</span><span class="line">}'</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-3">性能概述</h3>
<p>导入1个月的日志4.2G，31天的文件，每天一个索引，用了6个小时，elasticsearch用了 6.7G 的空间，在海量数据查询1s内响应。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Percona 小箱里的pt-archiver]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/01/21/percona-toolkitxiao-xiang-li-di-pt-archiver/"/>
    <updated>2015-01-21T23:27:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/01/21/percona-toolkitxiao-xiang-li-di-pt-archiver</id>
    <content type="html"><![CDATA[<p>在管理线上数据库，时常要做一些数据归档操作，在没有了解 <code>Percona toolkit</code> 之前，第一个想到的是在夜深人静的时候使用 <code>MySqlDump</code> 来完成这件事情。但它不是我们的优质选择，理由有：</p>

<ul>
  <li><code>MySqlDump</code> 只能备份在本机，不能直接做远端备份</li>
  <li>导出数据量太大的时候会锁表, 即使它的速度很快，但是在线上服务这是很危险的操作</li>
  <li>它仅仅只能导出，无法做到同时删除(可能不是太有必要)</li>
</ul>

<p>面对上述的场景，<code>Percona Toolkit</code> 让DBA 有了更好地选择，<a href="http://www.percona.com/doc/percona-toolkit/2.1/pt-archiver.html">pt-archiver</a> 应时而生。</p>

<h2 id="pt-archiver-">pt-archiver 介绍：</h2>

<p>根据官方文档的说法，几乎不会对线上的OTLP操作有影响：</p>

<blockquote>
  <p>The goal is a low-impact, forward-only job to nibble old data out of the table without impacting OLTP queries much</p>
</blockquote>

<p>它可以帮助我们将数据归档到文件, 另一个数据库，或者同一个数据库的另一个表, 亦或是用于合并两个表的内容。</p>

<h2 id="section">用法介绍：</h2>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">pt-archiver [OPTION...] --source DSN --where WHERE</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>归档的文件方便使用 load data infile 命令导入数据。另外你还可以用它来执行 delete 操作。这个工具默认的会删除源中的数据，使用的时候请注意。</p>

<p>假如我们将数据库里符合条件的记录归档到文件，并不做删除操作。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">pt-archiver --ask-pass --progress 100000 --no-delete --no-check-charset --source h=localhost,u=root,D=blog,t=comment--file /home/ubuntu/tmp/comment--where 'time &lt; "2013-12-31h"'</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>注意的选项参数：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">--ask-pass        提示要求输密码
</span><span class="line">--progress num    执行num行在界面通知我们
</span><span class="line">--source h,D,t    数据源
</span><span class="line">--no-delete       加上这个参数并不会执行删除操作
</span><span class="line">--dry-run         仅仅将执行语句打印在终端，事实上并不执行。可以用于检测执行过程
</span><span class="line">--where           执行语句，需要用冒号包围起来
</span><span class="line">--limit           批量操作的数量，合理提高这个数值可以加快archive速度</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-1">小总结</h3>

<p>通过开启 mysql 的 <code>general log</code>, 可以发现pt-archiver 执行时，是分批commit 的事务，因此执行效率会慢，在8核16G 内存的生产环境机器备份 1kw 条记录, 耗时150 分钟。 但基本不对服务造成影响，而且可以不用深夜进行, 值得一用。</p>

<p>最近攒了好多好工具和经验，要好好整理搬上来和大家分享才是。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[nginx错误码]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/12/13/nginxcuo-wu-ma/"/>
    <updated>2014-12-13T15:18:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/12/13/nginxcuo-wu-ma</id>
    <content type="html"><![CDATA[<p>在定位线上服务问题的时候，通常会去查看<code>Nginx</code> 的<code>error log</code></p>

<p>那么 error 的定义, 对查找问题就显得很有帮助</p>

<ul>
  <li>upstream prematurely closed connection</li>
</ul>

<blockquote>
  <p>请求uri的时候出现的异常，是由于 upstream 还未返回应答给用户时用户断掉连接造成的，对系统没有影响，可以忽略</p>
</blockquote>

<ul>
  <li>recv() failed (104: Connection reset by peer) </li>
</ul>

<blockquote>
  <p>服务器的并发连接数超过了其承载量，服务器会将其中一些连接Down掉；客户关掉了浏览器，而服务器还在给客户端发送数据;</p>
</blockquote>

<ul>
  <li>(111: Connection refused) while connecting to upstream </li>
</ul>

<blockquote>
  <p>用户在连接时，若遇到后端 upstream 挂掉或者不通，会收到该错误</p>
</blockquote>

<ul>
  <li>(111: Connection refused) while reading response header from upstream </li>
</ul>

<blockquote>
  <p>用户在连接成功后读取数据时，若遇到后端 upstream 挂掉或者不通，会收到该错误</p>
</blockquote>

<ul>
  <li>(111: Connection refused) while sending request to upstream </li>
</ul>

<blockquote>
  <p>Nginx 和 upstream 连接成功后发送数据时，若遇到后端 upstream 挂掉或者不通，会收到该错误</p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while connecting to upstream </li>
</ul>

<blockquote>
  <p>nginx 连接后面的 upstream 时超时</p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while reading upstream </li>
</ul>

<blockquote>
  <p>nginx 读取来自 upstream 的响应时超时 </p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while reading response header from upstream </li>
</ul>

<blockquote>
  <p>nginx 读取来自 upstream 的响应头时超时</p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while reading upstream </li>
</ul>

<blockquote>
  <p>nginx读取来自 upstream 的响应时超时</p>
</blockquote>

<ul>
  <li>(104: Connection reset by peer) while connecting to upstream </li>
</ul>

<blockquote>
  <p>upstream发送了 RST，将连接重置</p>
</blockquote>

<ul>
  <li>upstream sent invalid header while reading response header from upstream </li>
</ul>

<blockquote>
  <p>upstream 发送的响应头无效</p>
</blockquote>

<ul>
  <li>upstream sent no valid HTTP/1.0 header while reading response header from upstream</li>
</ul>

<blockquote>
  <p>upstream 发送的响应头无效</p>
</blockquote>

<ul>
  <li>client intended to send too large body </li>
</ul>

<blockquote>
  <p>用于设置允许接受的客户端请求内容的最大值，默认值是1M，client 发送的 body 超过了设置值</p>
</blockquote>

<ul>
  <li>reopening logs </li>
</ul>

<blockquote>
  <p>用户发送kill  -USR1命令</p>
</blockquote>

<ul>
  <li>gracefully shutting down</li>
</ul>

<blockquote>
  <p>用户发送kill  -WINCH命令</p>
</blockquote>

<ul>
  <li>no live upstreams while connecting to upstream </li>
</ul>

<blockquote>
  <p>upstream 下的 server 全都挂了</p>
</blockquote>

<ul>
  <li>SSL_do_handshake() failed</li>
</ul>

<blockquote>
  <p>SSL握手失败</p>
</blockquote>

<ul>
  <li>ngx_slab_alloc() failed: no memory in SSL session shared cache</li>
</ul>

<blockquote>
  <p>ssl_session_cache大小不够等原因造成</p>
</blockquote>

<ul>
  <li>could not add new SSL session to the session cache while SSL handshaking</li>
</ul>

<blockquote>
  <p>ssl_session_cache 大小不够等原因造成</p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用到的Tcpdump]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/11/29/yong-dao-de-tcpdump/"/>
    <updated>2014-11-29T14:15:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/11/29/yong-dao-de-tcpdump</id>
    <content type="html"><![CDATA[<p>开发中，要定位具体问题，特别是网络问题的时候，多数是要晋出<code>tcpdump</code>，遗憾的是我略懂皮毛，有必要深入一些。
简单说下我常用的 TcpDump的方法</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">tcpdump -i eth0 -Xxn port 80 -s 0 -c 1024 </span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如果仅仅是看manual  多数时候还是会忘记，好记性不如烂笔头，上述的选项是我认为很有用的</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">-i 指定网卡
</span><span class="line">-Xxn X使用Ascii和16进制，n 表示 ip 用数字表示
</span><span class="line">-s 0 表示整包抓取
</span><span class="line">-c 1024 表示包得大小</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如果希望将抓包过程中保留下来，可以在上述命令尾部加上 <code>-w trace.cap</code>
这种格式的文件，文本编辑器是无法理解，需要特殊的软件才能回复，比如 <code>wireshark</code></p>

<p>Tcpdump 中的 flag 有必要提下：</p>

<ul>
  <li>PSH 代表要求发送立即发送缓冲区内的其他对应数据包，无需缓冲区满才发送</li>
  <li>RST 如果RST=1表示连接马上结束，无需等待终止确认手续，发送端已经断线</li>
  <li>SYNC 表示主动连接到对方，建立连接</li>
  <li>FIN 表示传送结束，发送方等待对方响应</li>
</ul>

<p>通过 wireshark 可以再现所谓的三次握手和四次挥手过程。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[慢]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/11/25/tired/"/>
    <updated>2014-11-25T22:20:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/11/25/tired</id>
    <content type="html"><![CDATA[<p>这三个月以来,有30天是凌晨才离开公司的.甚至有好几天都通宵没有睡过,累。</p>

<p>貌似熟悉了凌晨的路灯,和冷冽的寒风。</p>

<p>技术的东西,整理在 EvertNote 里，但想写一些非技术的冲动似乎更强。</p>

<p>生活有很长的路要走, 几乎每天都在发现不足之处,只能让内心更强大, 去接受,去改变, 淡定去拥抱无时不刻的变化, 怀着信念与希望继续走着。</p>

<p>感谢 Everet 一直以来给我的信念, 我们也都在《感谢你让我上场》获得共鸣。我也坚信这位青年会是我们这届的骄傲。</p>

<p>我想让生活慢下来，做一些重要不紧急的事。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx与php-fpm系统参数配置]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/11/02/nginxyu-phpxi-tong-can-shu-pei-zhi/"/>
    <updated>2014-11-02T15:46:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/11/02/nginxyu-phpxi-tong-can-shu-pei-zhi</id>
    <content type="html"><![CDATA[<p>需要为机器调配参数了, 早些时候看过不少文章, 但没经历过始终不深刻, 以下讲的是在 8core 8G，Centos 配置 php-fpm 与 NGINX。</p>

<h3 id="php-fpm-">php-fpm 参数配置</h3>

<p>关于 php-fpm 的配置,Ubuntu系统中 <code>/etc/php5/fpm/pool.d</code>目录下, Centos是<code>/etc/php-fpm.d</code>目录下编辑<code>www.conf</code></p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">listen = /var/run/php5-fpm.sock
</span><span class="line">listen.backlog = -1 (on FreeBSD -1 unlimit)
</span><span class="line">pm = static # 使用静态进程管理
</span><span class="line">pm.max_children = 128
</span><span class="line">request_terminate_timeout = 8s #设置太大，会导致work进程过多，来不及kill掉</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>编辑 <code>/etc/default/php5-fpm</code></p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">ulimit -n 655360</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="nginx-">Nginx 配合参数</h3>

<ul>
  <li>启用irqbalance</li>
</ul>

<p>由于精简系统的服务没有开启irqbalance，irqbalance现在被证实为非常有必要的服务，他的主要功能是可以合理的调配使用各个 CPU 核心，特别是对于目前主流多核心的 CPU，简单的说就是能够把压力均匀的分配到各个 CPU 核心上，对提升性能有很大的帮助。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">shell&gt; yum -y install irqbalance
</span><span class="line">shell&gt; service irqbalance start
</span><span class="line">cat /proc/irqbalance #查看中断的分布</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>为nginx 绑定 cpu</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">worker_rlimit_nofile 300000;
</span><span class="line">worker_processes  8;
</span><span class="line">worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000;</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>连接数调整，</li>
</ul>

<p>nginx发起的连接数，远远超过了 php-fpm 所能处理的数目，导致端口（或socket）频繁被锁，造成堵塞。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class=""><span class="line">vi /etc/sysctl.conf 进行了微调
</span><span class="line">fs.file-max = 6553600
</span><span class="line">
</span><span class="line">vim /etc/security/limits.conf
</span><span class="line">* soft nofile 655360
</span><span class="line">* hard nofile 655360
</span><span class="line">
</span><span class="line">vim /etc/nginx/nginx.conf
</span><span class="line">
</span><span class="line">worker_rlimit_nofile 300000;
</span><span class="line">events {
</span><span class="line">    worker_connections 300000;
</span><span class="line">    use epoll;
</span><span class="line">}
</span><span class="line">http {
</span><span class="line">    keepalive_timeout  0; #关闭keepalive_timeout, 快速释放系统资源
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>从查到的资料看起来 8 核 理論值的最大連線數 = <code>worker_processes * worker_connections / 8</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iptable做NAT转发]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/29/iptablezuo-natzhuan-fa/"/>
    <updated>2014-10-29T22:38:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/29/iptablezuo-natzhuan-fa</id>
    <content type="html"><![CDATA[<p>简单简述下我遇到的问题:</p>

<p>现在局域网有2台机器, 其中一台机器(下文我们称之为ServerA)可以访问外网,有独立IP,而另外一台机器(ServerB)访问不了外网, 需要想办法让 ServerB 也能上网。</p>

<ul>
  <li>ServerA 操作</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">vi /etc/sysctl.conf
</span><span class="line">net.ipv4.ip_forward = 1
</span><span class="line">iptables -t nat -A POSTROUTING -s 10.4.0.0/16 -j MASQUERADE</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>ServerB</p>

<p>编辑 /etc/network/interface
主要是修改gateway 参数，指向 ServerA 的 IP</p>

<p><code>
sudo ifdown eth0; sudo ifup eth0</code> 
ServerB 就可以连接外网了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[唯一索引引发的思考]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/23/mysql-unique-index/"/>
    <updated>2014-10-23T23:18:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/23/mysql-unique-index</id>
    <content type="html"><![CDATA[<p>最近需要改动线上一个有千万条记录的表，涉及到加字段操作，这个表有索引，按照经验，为了加速修改表结构,去掉索引。由于我删除的是Unique Index, 而服务一直在写,程序依赖数据库的唯一索引去重,导致瞬间有重复数据,唯一索引重新加上时会报 <code>Duplicated Key entry Error</code>,</p>

<h3 id="section">回放事件</h3>

<p>我们的数据表，之前是有 <code>UNIQUE INDEX(cuid, aid)</code>, 因为去掉索引，服务持续写入，导致有重复记录，所幸的是，这是一个统计表, 不影响功能，所以需要找出重复的记录</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">select cuid,aid from (
</span><span class="line">    select cuid, aid,count(1) as num
</span><span class="line">    from register_chn 
</span><span class="line">    group by cuid,aid having num &gt; 1
</span><span class="line">) t;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>显示有8条记录,如果手动删除,是很慢且愚蠢的做法,还是用 SQL 执行,镇定之后执行</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">delete from register_chn where (cuid,aid) in (
</span><span class="line">    select cuid,aid from (
</span><span class="line">        select cuid, aid,count(1) as num
</span><span class="line">        from register_chn
</span><span class="line">        group by cuid,aid having num &gt; 1
</span><span class="line">    ) t
</span><span class="line">);</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>影响了８条记录.然后瞬间加上索引.所幸是成功了, 事实上当时的合理操作应该是用事务。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">BEGIN;
</span><span class="line">delete from register_chn where (cuid,aid) in (
</span><span class="line">    select cuid,aid from (
</span><span class="line">        select cuid, aid,count(1) as num
</span><span class="line">        from register_chn
</span><span class="line">        group by cuid,aid having num &gt; 1
</span><span class="line">    ) t
</span><span class="line">);
</span><span class="line">alter table register_chn add unique index(cuid, aid);
</span><span class="line">COMMINT;</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section-1">引发的思考</h3>

<p>后来想想, 上述方法虽然解决问题了, 但是有点碰运气成分。如果频繁快速地产生重复记录,也许就没那么好运了,事实上可以执行以下 SQL:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">alter ignore table register_chn add unique index(cuid, aid)；</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>如果你以为很简单,那就错了。这个方法在 MySQL 5.0 上使用是没问题的，但是在5.6 之前是有bug的，亲自测试Percona 版本5.5, 的确会失败
官方的解决方法是：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">set session old_alter_table = on;
</span><span class="line">alter ignore table register_chn add unique index(cuid, aid);</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>在有主备的情况,记得执行前设置一下</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">set session sql_log_bin=off. </span></code></pre></td></tr></table></div></figure></notextile></div>
<p>以免备库报错。同样还需要在备库重复一下主库的操作, 这也算是一个不太完美的解决思路。</p>

]]></content>
  </entry>
  
</feed>
