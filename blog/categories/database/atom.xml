<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: DataBase | 织网]]></title>
  <link href="http://zheng-ji.github.com/blog/categories/database/atom.xml" rel="self"/>
  <link href="http://zheng-ji.github.com/"/>
  <updated>2014-11-25T22:55:40+08:00</updated>
  <id>http://zheng-ji.github.com/</id>
  <author>
    <name><![CDATA[zheng-ji]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[唯一索引引发的思考]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/23/mysql-unique-index/"/>
    <updated>2014-10-23T23:18:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/23/mysql-unique-index</id>
    <content type="html"><![CDATA[<p>最近需要改动线上一个有千万条记录的表，涉及到加字段操作，这个表有索引，按照经验，为了加速修改表结构,去掉索引。由于我删除的是Unique Index, 而服务一直在写,程序依赖数据库的唯一索引去重,导致瞬间有重复数据,唯一索引重新加上时会报 <code>Duplicated Key entry Error</code>,</p>

<h3 id="section">回放事件</h3>

<p>我们的数据表，之前是有 <code>UNIQUE INDEX(cuid, aid)</code>, 因为去掉索引，服务持续写入，导致有重复记录，所幸的是，这是一个统计表, 不影响功能，所以需要找出重复的记录</p>

<p><code>
select cuid,aid from (
    select cuid, aid,count(1) as num
    from register_chn 
    group by cuid,aid having num &gt; 1
) t;
</code></p>

<p>显示有8条记录,如果手动删除,是很慢且愚蠢的做法,还是用 SQL 执行,镇定之后执行</p>

<p><code>
delete from register_chn where (cuid,aid) in (
    select cuid,aid from (
        select cuid, aid,count(1) as num
        from register_chn
        group by cuid,aid having num &gt; 1
    ) t
);
</code>
影响了８条记录.然后瞬间加上索引.所幸是成功了, 事实上当时的合理操作应该是用事务。</p>

<p><code>
BEGIN;
delete from register_chn where (cuid,aid) in (
    select cuid,aid from (
        select cuid, aid,count(1) as num
        from register_chn
        group by cuid,aid having num &gt; 1
    ) t
);
alter table register_chn add unique index(cuid, aid);
COMMINT;
</code></p>

<h3 id="section-1">引发的思考</h3>

<p>后来想想, 上述方法虽然解决问题了, 但是有点碰运气成分。如果频繁快速地产生重复记录,也许就没那么好运了,事实上可以执行以下 SQL:</p>

<p><code>
alter ignore table register_chn add unique index(cuid, aid)；
</code></p>

<p>如果你以为很简单,那就错了。这个方法在 MySQL 5.0 上使用是没问题的，但是在5.6 之前是有bug的，亲自测试Percona 版本5.5, 的确会失败
官方的解决方法是：</p>

<p><code>
set session old_alter_table = on;
alter ignore table register_chn add unique index(cuid, aid);
</code></p>

<p>在有主备的情况,记得执行前设置一下</p>

<p><code>
set session sql_log_bin=off. 
</code>
以免备库报错。同样还需要在备库重复一下主库的操作, 这也算是一个不太完美的解决思路。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mysql的一些配置]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/21/mysqlde-na-xie-dong-tai-bian-liang/"/>
    <updated>2014-10-21T23:18:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/21/mysqlde-na-xie-dong-tai-bian-liang</id>
    <content type="html"><![CDATA[<p>我想写一篇长期更新的文章，随着知识积累持续更新的文章，而最近遇到的在涉及的Mysql优化，便是一个可以琢磨的点，变量比较多，并且需要在每次采坑才来的深刻。</p>

<ul>
  <li>tmp_table_size
这个是临时表的大小，与之对应的有 <code>max_heap_size</code> 单位是Byte, <code>tmp_table_size</code> 的大小关乎临时表的大小, 如果设置太小，会导致一些数据直接写入磁盘，比较蛋疼, 看了官方文档，涉及到<code>Dynamic Variable=Yes</code>
那么这种配置便可以直接在终端用语句设置，并生效，不需要重启机器</li>
</ul>

<p><code>
set global tmp_table_size = 512 * 1024 *1024 
</code></p>

<ul>
  <li>max_connections
当<code>show processlist</code> 发现有好多链接的时候，客户端想登陆mysql却失败。可以执行, 看看是不是现有的链接数已经逼近</li>
</ul>

<p><code>
show global variable like '%max%'
set global max_connections 1000
</code>
* thread_concurrency</p>

<p>同一时间与性的线程设置，如果分配不当，会导致<code>mysql</code>不能充分使用多｀cpu｀, 出现同一时刻只有一个核在工作。
<code>thread_concurrency</code> 应设为<code>CPU</code> 核数的2倍. 比如有一个双核的CPU, 那么<code>thread_concurrency</code>的应该为4; 2个双核的cpu, <code>thread_concurrency</code>的值应为8.</p>

<ul>
  <li>max_allowed_packet
如果需要进行mysqldump这类操作，需要调大</li>
</ul>

<p><code>
set global max_allowed_packet = 2*1024*1024*10
</code></p>

<ul>
  <li>skip_slave_start
配置文件中建议加上skip-slave-start，以免在不需要时候slave线程自己开始执行了</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[redis热备]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/04/redisre-bei/"/>
    <updated>2014-10-04T22:28:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/04/redisre-bei</id>
    <content type="html"><![CDATA[<p>一转眼到了 10 月份了, 依然很忙, 好多碰到的，解决了的, 没能解决的蛋疼问题, 那些一闪而过的被记载于 evernote里, 信誓旦旦说过要记载下来，终究也还是被时间欺骗。 </p>

<p>在过去的九月份，我们迁移了一批业务到云上。除了 mysql 需要热备处理，redis-server 同样也面临这种问题。相比起来，redis 的热备显得更为简单。</p>

<p>为了可以更好地阐述过程，我们定义了三个角色</p>

<ul>
  <li>A 服务器，原来拥有 redis 数据库的机器 </li>
  <li>B 服务器，未来取代 A 服务器的 redis 数据库服务器</li>
  <li>
    <p>C 服务器，B 服务器的 slave</p>
  </li>
  <li>我们在停止服务的5分钟，在 A 的命令行，执行</li>
</ul>

<p><code>
bgsave
</code></p>

<p>使得内存里的数据，完整地保存于 dumps.rdb</p>

<ul>
  <li>
    <p>配置好 B 服务器的redis server, 用上述的 dumps.rdb 取代该服务器的 dumps.rdb , 这里说的配置好，需要指定好 bind-address, 推荐使用内网 IP, 备份目录</p>
  </li>
  <li>
    <p>在 C 服务器的 redis 配置上 更改为:</p>
  </li>
</ul>

<p><code>
slaveof  IP地址 端口
slave-serve-stale-data no
slave-read-only yes # 只读
</code></p>

<p>这样就开始备份了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Percona Server 做主从]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/09/07/percona-server-zuo-zhu-cong/"/>
    <updated>2014-09-07T15:31:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/09/07/percona-server-zuo-zhu-cong</id>
    <content type="html"><![CDATA[<p>数据库做主从，这个过程是需要很耐心的。</p>

<p>数据库做主从目的:</p>

<ul>
  <li>故障恢复， 柔性可用</li>
  <li>也可以做读写分离</li>
</ul>

<p>实验过程中 mysql 用的版本是 Percona Server ，</p>

<p><a href="http://www.percona.com/doc/percona-server/5.5/installation/apt_repo.html">安装过程</a></p>

<p>由于修改默认的数据目录，数据文件不再使用 <code>/var/lib/mysql</code>,数据文件夹被我安放位置是 <code>/data/mysql/data</code> 同时log 目录也放在这里 <code>/data/mysql/log</code> 
注意需要修改目录属主</p>

<p><code>
sudo chown mysql:mysql  -R /data/mysql
然后执行 sudo mysql_install_db 
</code></p>

<p>这时候会发生 <code>sudo service mysql stop</code> 失败，原因和方法见此 <a href="http://serverfault.com/questions/32692/cant-start-stop-mysql-service/420222#420222">神贴</a></p>

<p>好了开始进入正题了，备份数据的原理</p>

<ul>
  <li>在主库上把数据更改记录到二进制日志 (Binary Log) </li>
  <li>备库将主库的日志复制到自己的中继日志中</li>
  <li>备库读取中继日志的事件，将其重放到备库的数据之上</li>
</ul>

<p>从其他服务器克隆备库的方法：</p>

<ul>
  <li>使用冷备份：关闭主库，将数据复制到备库，重启主库后，会使用一个新的二进制日志文件，在备库执行 CHANGE MASTER TO 指向这个文件的起始处，缺陷在于关闭主库</li>
  <li>使用热备份，如果仅使用，mysqlhotcopy,或rsync来复制数据</li>
  <li>使用快照或备份：需要知道二进制日志坐标，就可以使用主库的快照和备份来初始化备库，只需要把备份或快照恢复到备库，然后使用 CHANGE MASTER TO 指定二进制日志的坐标.</li>
  <li>用<code>Percona Xtrabackup</code>  个人推荐  <a href="http://www.percona.com/doc/percona-xtrabackup/2.1/howtos/setting_up_replication.html">链接</a></li>
</ul>

<p>======= 实际的步骤如下 ====</p>

<ul>
  <li>备份主库的数据</li>
</ul>

<p><code>
innobackupex --user=root --password=xxx /home/zj/backup
</code>
在/home/zj/backup目录下就生成了2014-08-21_10-11-4` 目录</p>

<ul>
  <li>复制数据到从库，通过<code>scp</code>将上一步生成的目录放置在从库机器(~/tmp`)将原来的data目录备份, 在从库机器执行</li>
</ul>

<p><code>
mv /data/mysql/data /data/mysql/data_bak
mv ~/tmp/2014-08-21_10-11-46  /data/mysql/data
chown mysql:mysql -R /data/mysql/data
</code></p>

<ul>
  <li>配置主服务器</li>
</ul>

<p><code>
GRANT REPLICATION SLAVE ON *.*  TO 'repl'@'$slaveip' IDENTIFIED BY ''slavepass
</code></p>

<ul>
  <li>配置从数据库,并重启</li>
</ul>

<p><code>
server-id=2
</code></p>

<ul>
  <li>开始复制</li>
</ul>

<p>需要定位位置</p>

<p><code>
cat /data/mysql/data/xtrabackup_binlog_info
log-bin.000001     481
</code></p>

<ul>
  <li>在从库执行 </li>
</ul>

<p><code>
mysql&gt; CHANGE MASTER TO MASTER_HOST='$masterip',
       MASTER_USER='repl',
       MASTER_PASSWORD='$slavepass',
       MASTER_LOG_FILE='log-bin.000001',
       MASTER_LOG_POS=481;
mysql&gt; START SLAVE;
mysql&gt; SHOW SLAVE STATUS \G
</code></p>

<hr />

<p>系统学习的书籍，感谢杨先生推荐 : )
<a href="http://book.douban.com/subject/23008813/">高性能mysql</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[最近用到的mysql]]></title>
    <link href="http://zheng-ji.github.com/blog/2013/12/05/zui-jin-yong-dao-de-mysql/"/>
    <updated>2013-12-05T23:01:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2013/12/05/zui-jin-yong-dao-de-mysql</id>
    <content type="html"><![CDATA[<p>该文章没有什么创造性营养，也没有什么技术沉淀，各位看官轻轻带过，仅仅经验之谈，看过用过的人都会，简单的事情就平滑地带过。怎么让索然无味的文章更好地被他人深入阅读,要么就是实用，要么就是深刻，不扯淡, 直奔主题。</p>

<h4 id="section">查看表结构</h4>

<p><code>
desc Tbl;
show create table Tbl;
show full fields table Tbl;
</code></p>

<p>如果表有很多分区，导致很多打点刷屏，那么可以用</p>

<p><code>
desc  Tbl/G
</code></p>

<p>表太多的话，我的场景是2000张表,:(</p>

<p><code>
SHOW TABLES LIKE '%tb%';
</code></p>

<h4 id="section-1">查询数据库的字符集</h4>

<p><code>
show variables like '%character%';
</code></p>

<h4 id="section-2">指定字符集直连</h4>

<p><code>
mysql -h10.145.135.234 -uoss -pxxx dbName --default-character-set=gbk;
</code></p>

<h4 id="section-3">赋予权限</h4>

<p><code>
show grants for 'xxx'@'xxx.xxx.xxx.xxx'
</code></p>

<h4 id="section-4">如果想备份，或者复制表</h4>

<p><code>
create table newtb select * from oldtable
</code></p>

<h4 id="section-5">修改表字段</h4>

<p><code>
alter tbA modify fieldA int(11) unsigned default '0'
</code></p>

<h4 id="section-6">移除字段</h4>

<p><code>
alter table t2 drop column c;
</code></p>

<h4 id="section-7">修改主键</h4>

<p><code>
alter table tbA drop primary key;
Alter table tbA add primary key(dtStatDate,File2)
</code></p>

<h4 id="query">查询当前数据库的查询进程,干掉挂起的query</h4>

<p><code>
show processlist;
kill pid
</code></p>

<h4 id="shell">直接在shell外部命令行执行:</h4>

<p><code>
mysql -h10.112.111.111 -uxxx -pxxxx dbname -Ns -e"
select distinct(iUin) from TableA where dtEventTime &gt;= '2013-05-05' 
" &gt; records.txt;
</code></p>

<h4 id="mysql--">mysql 排除重复记录 使用</h4>

<p><code>
INSERT IGNORE into
Replace Into 是为了让主键替换原有的记录
</code></p>

<h4 id="load-data-infile">load data infile语句</h4>
<p>从一个文本文件中以很高的速度读入一个表中。使用这个命令之前，mysqld进程（服务）必须已经在运行。为了安全原因，当读取位于服务器上的文本文件时，文件必须处于数据库目录或可被所有人读取。另外，为了对服务器上文件使用load data infile，在服务器主机上你必须有file的权限。</p>

<p><code>
load data infile "/home/Order txt" into table Orders(Order_Number, Order_Date, Customer_ID) terminated by',';
</code></p>

<h4 id="mysqldump-access-denied-for-user-when-using-lock-tables">mysqldump中解决 报”Access denied for user when using LOCK TABLES”</h4>

<p><code>
mysqldump -udbuser -p dbname --skip-lock-tables &gt; dbname.sql
</code></p>

<h4 id="section-8">监控数据库磁盘增长量</h4>

<p><code>
use information_schema;
#库的大小
select concat(round(sum(DATA_LENGTH/1024/1024),2),'MB') as data  from TABLES where table_schema='db1';
</code></p>

<h4 id="section-9">时间函数</h4>

<p><code>
FROM_UNIXTIME(iLoginTime,'%Y-%m-%d')
unix_timestamp('2013-12-03');
Date_SUB(OrderDate,INTERVAL 2 DAY)
Date_ADD(OrderDate,INTERVAL 2 DAY)
timestampdiff(week,’2009-01-24′,’2009-06-20′);
</code></p>

<h4 id="no-existhaving">no exist,having</h4>
<p>适合做留存用户分析</p>

<p>```
SELECT distinct field1
FROM table a
where date &lt;= 20130430 and
not exists (
        SELECT field1 FROM table b where date&gt;=20130501 and a.field1=b.field1
)</p>

<h1 id="section-10">举例子,可以计算第一次创建用户的人</h1>
<p>select * from TbRole having min(CreateTime) between ‘xxx’ and ‘xxx’ 
```</p>

<h4 id="mysql-">mysql 逻辑运算,好像在编程的样子</h4>

<p>```
case condition1
when result1 then ‘xx’
when result2 then ‘xx’
else result1 then ‘xx’
end as ‘xx’</p>

<p>ifnull(a,b) #如果a不为空的情况下执行a,否则执行b
if(condition,a,b)#如果符合condition 执行a,否则执行b
```</p>

<h4 id="group-by">妙用group by</h4>
<p>可以在group by 里面做逻辑运算, 缩小逻辑范围</p>

<p><code>
select * From tableA group by (round(field1/100,0));
</code></p>

<h4 id="section-11">格式化字符串类型</h4>

<p><code>
concat('a','b')
</code></p>

<h4 id="mysql">mysql的临时变量’@’</h4>
<p>以下sql是取各道具排前10的等级</p>

<p><code>
select * from (
    select dtStatDate,iRoleLevel,iUserNum,
    if(@templevel=a.iRoleLevel,@tno:=@tno+1,@tno:=1) as tno,@templevel:=a.iRoleLevel 
    from tbl_a , (SELECT @tno:= 0,@templevel:=null) tbl_b
    order by a.iRoleLevel asc,a.iUserNum desc
) c
where tno&lt;=10 order by iRoleLevel asc
</code></p>

<h4 id="section-12">其他</h4>
<p>union all 效率会比union高,因为union有去重功能,会耗时 </p>
]]></content>
  </entry>
  
</feed>
