<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Server | 织网]]></title>
  <link href="http://zheng-ji.github.com/blog/categories/server/atom.xml" rel="self"/>
  <link href="http://zheng-ji.github.com/"/>
  <updated>2017-06-04T13:34:14+08:00</updated>
  <id>http://zheng-ji.github.com/</id>
  <author>
    <name><![CDATA[zheng-ji]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[zookeeper 笔记]]></title>
    <link href="http://zheng-ji.github.com/blog/2017/02/25/zookeeper-bi-ji/"/>
    <updated>2017-02-25T17:30:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2017/02/25/zookeeper-bi-ji</id>
    <content type="html"><![CDATA[<p>ZooKeeper 是一个开源的分布式协调服务，是分布式数据一致性的解决方案。</p>

<h3 id="section">集群角色</h3>

<p>在 ZooKeeper 中，有三种角色： Leader，Follower，Observer</p>

<p>一个 ZooKeeper 集群同时只会有一个Leader，其他都是 Follower 或 Observer。
Leader 服务器为客户端提供读和写服务，Follower 和 Observer 都能提供读服务，不能提供写服务。区别在于，Observer 不参与 Leader 选举过程，也不参与写操作的过半写成功策略，因此 Observer 可以在不影响写性能的情况下提升集群的读性能。</p>

<h3 id="section-1">会话</h3>

<p>客户端和 ZooKeeper 服务器会与服务器建立一个 TCP 连接，通过这个连接，客户端能够通过心跳检测和服务器保持有效的会话，也能够向 ZooKeeper 服务器发送请求并接受响应，同时还能通过该连接接收来自服务器的 Watch 事件通知。</p>

<h3 id="section-2">数据节点</h3>

<p>ZooKeeper 中的数据节点是指数据模型中的数据单元，称为 ZNode。ZooKeeper将所有数据存储在内存中，数据模型是一棵树（ZNode Tree），由斜杠进行分割的路径，就是一个ZNode。每个ZNode上都会保存自己的数据内容，同时会保存一系列属性信息。每个ZNode不仅本身可以写数据，还可以有下一级文件或目录。</p>

<p>在ZooKeeper中，ZNode可以分为持久节点和临时节点两类。持久节点是指一旦这个 ZNode 被创建了，除非主动进行 ZNode 的移除操作，否则这个 ZNode 将一直保存在 ZooKeeper上。临时节点的生命周期跟客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。</p>

<p>ZooKeeper 允许用户为每个节点添加一个特殊的属性：SEQUENTIAL。一旦节点被标记上这个属性，那么在这个节点被创建的时候，ZooKeeper就会自动在其节点后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。</p>

<h3 id="section-3">版本</h3>

<p>ZooKeeper 的每个 ZNode 上都会存储数据，对于每个ZNode，ZooKeeper都会为其维护一个叫作Stat的数据结构，Stat中记录了这个ZNode的三个数据版本，分别是 version（当前ZNode的版本, cversion（当前ZNode子节点的版本）和 aversion（当前ZNode的ACL版本）。</p>

<h3 id="section-4">事务</h3>

<p>在 ZooKeeper 中，能改变 ZooKeeper 服务器状态的操作称为事务操作。包括数据节点创建与删除、数据内容更新和客户端会话创建与失效等操作。对应每一个事务请求，ZooKeeper都会为其分配一个全局唯一的事务ID，用ZXID表示，通常是一个64位的数字。每一个ZXID对应一次更新操作，从这些ZXID中可以间接地识别出ZooKeeper处理这些事务操作请求的全局顺序。</p>

<h3 id="watcher">Watcher</h3>

<p>ZooKeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去。该机制是 ZooKeeper 实现分布式协调服务的重要特性。</p>

<h3 id="acl">ACL</h3>

<p>ZooKeeper 采用 Access Control Lists 策略来进行权限控制。ZooKeeper 定义了如下5种权限。</p>

<p><code>
CREATE: 创建子节点的权限。
READ: 获取节点数据和子节点列表的权限。
WRITE：更新节点数据的权限。
DELETE: 删除子节点的权限。
ADMIN: 设置节点ACL的权限。
注意：CREATE 和 DELETE 都是针对子节点的权限控制。
</code></p>

<h3 id="zab-">ZAB 原子广播协议</h3>

<p>ZooKeeper Atomic Broadcast（ZAB，ZooKeeper原子广播协议）的协议作为其数据一致性的核心算法。</p>

<p>所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为Leader服务器，而剩下的其他服务器则成为 Follower 服务器。Leader 服务器负责将一个客户端事务请求转换成一个事务 Proposal（提案）并将该 Proposal分发给集群中所有的 Follower 服务器。之后 Leader 服务器需要等待所有 Follower 服务器的反馈，一旦超过半数的 Follower 服务器进行了正确的反馈后，Leader 就会再次向所有的 Follower 服务器分发 Commit 消息，要求对刚才的 Proposal 进行提交。</p>

<h3 id="section-5">应用场景</h3>

<ul>
  <li>数据发布与订阅-配置中心</li>
</ul>

<p>发布者将数据发布到 ZooKeeper 节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和动态更新。全局配置信息就可以发布到 ZooKeeper 上，让客户端（集群的机器）去订阅该消息。</p>

<p>客户端想服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应的客户端发送 Watcher 事件通知，客户端接收到这个消息通知后，需要主动到服务端获取最新的数据（推拉结合）。</p>

<ul>
  <li>
    <p>命名服务，即生成全局唯一的ID。</p>
  </li>
  <li>
    <p>分布式协调通知</p>
  </li>
</ul>

<p>ZooKeeper 中特有 Watcher 注册与异步通知机制，实现对数据变更的实时处理。不同的客户端都对ZK上同一个ZNode 进行注册，监听 ZNode 的变化（包括ZNode本身内容及子节点的），如果 ZNode 发生了变化，那么所有订阅的客户端都能够接收到相应的 Watcher 通知，并做出相应的处理，是一种通用的分布式系统机器间的通信方式。</p>

<ul>
  <li>心跳检测</li>
</ul>

<p>基于 ZK 临时节点的特性，可以让不同的进程都在 ZK 的一个指定节点下创建临时子节点，不同的进程直接可以根据这个临时子节点来判断对应的进程是否存活。通过这种方式，检测和被检测系统直接并不需要直接相关联，而是通过 ZK 上的某个节点进行关联，大大减少了系统耦合。</p>

<ul>
  <li>分布式锁</li>
</ul>

<p>分布式锁是控制分布式系统之间同步访问共享资源的一种方式。分布式锁又分为排他锁和共享锁两种。 排他锁又称为写锁或独占锁，共享锁又称为读锁。</p>

<p>把 ZooKeeper 上一个 ZNode 看作是一个锁，获得锁就通过创建ZNode的方式来实现。所有客户端都去/x_lock节点下创建临时子节点/x_lock/lock。ZooKeeper会保证在所有客户端中，最终只有一个客户端能够创建成功，那么就可以认为该客户端获得了锁。同时，所有没有获取到锁的客户端就需要到/x_lock节点上注册一个子节点变更的 Watcher 监听，以便实时监听到 lock 节点的变更情况。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Twemproxy 一个 Redis 代理]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/08/16/twemproxy/"/>
    <updated>2015-08-16T12:11:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/08/16/twemproxy</id>
    <content type="html"><![CDATA[<dl>
  <dt>为解决线上 Redis 服务直连出现链接数爆棚而做的调研， 对 Twitter 开源的 twemproxy 做一些记录。 我们之所以放弃官方的 RedisCLuster 是因为不太满意其性能</dt>
  <dd>
    <p><a href="#第一节">初窥原理</a>
* <a href="#第二节">安装与配置</a>
* <a href="#第三节">不支持的操作</a>
* <a href="#第四节">压力测试</a>
* <a href="#第五节">摘自极光博客的评论</a></p>
  </dd>
</dl>

<h3 id="第一节">初窥原理</h3>

<ul>
  <li>Twitter 出品的轻量级 Redis，memcached 代理，使用它可以减少缓存服务器的连接数，并且利用它来作分片。</li>
  <li>作是说最差情况下，性能损耗不会多于20%。背后是用了pipeline，redis是支持使用pipeline批处理的。</li>
  <li>twemproxy 与每个 redis 服务器都会建立一个连接，每个连接实现了两个 FIFO 的队列， 通过这两个队列实现对 redis 的 pipeline 访问，将多个客户端的访问合并到一个连接，这样既减少了redis服务器的连接数，又提高了访问性能。</li>
</ul>

<h3 id="第二节">安装与配置</h3>

<ul>
  <li>安装</li>
</ul>

<p><code>
apt-get install automake
apt-get install libtool
git clone git://github.com/twitter/twemproxy.git
cd twemproxy
autoreconf -fvi
./configure
make
sudo make install
</code>
默认的可执行文件在 /usr/local/sbin/nutcracker</p>

<ul>
  <li>配置文件 /etc/nutcracker/nutcracker.yml</li>
</ul>

<p><code>
alpha:
    listen: 127.0.0.1:8877
    hash: fnv1a_64
    distribution: ketama
    auto_eject_hosts: true
    redis: true
    server_retry_timeout: 30000
    server_failure_limit: 3
    servers:
        - 127.0.0.1:6379:1 master0  #后端的redis-server
        - 127.0.0.1:6380:1 master1
</code></p>

<p>当 redis 做缓存的使用的时候应该启用 auto_eject_hosts， 如果某个节点失败的时候将该节点删除，虽然丧失了数据的一致性，但作为缓存使用，保证了这个集群的高可用性。当redis做存储的使用时为了保持数据的一致性，应该禁用 auto_eject_hosts,也就是当某个节点失败之后并不删除该节点。</p>

<h3 id="第三节">不支持的操作</h3>

<p><code>
keys command: keys,migrate,move object,randomkey,rename,renamenx,
sort strings command: bitop,mset,msetnx
list command: blpop,brpop,brpoplpush
scripting command: script exists,script flush,script kill,script load
pub/sub command:(全部不支持)psubscribe,publish,punsubscribe,subscribe,unsubscribe
</code></p>

<h3 id="第四节">压测</h3>

<p>感谢 redis 提供的 redis-benchmark 工具，用它来做压测挺好的。</p>

<ul>
  <li>n 表示多少个连接</li>
  <li>r 表示多少个 key,</li>
  <li>t 代表命令</li>
</ul>

<p>```
zj@zheng-ji.info:~$ redis-benchmark -p 6700 -t smembers,hexists,get,hget,lrange,ltrim,zcard,setex,sadd -n 1000000 -r 100000000</p>

<p>====== GET ======
1000000 requests completed in 12.95 seconds
50 parallel clients
3 bytes payload
keep alive: 1</p>

<p>99.19% &lt;= 1 milliseconds
99.93% &lt;= 2 milliseconds
100.00% &lt;= 2 milliseconds
77220.08 requests per second</p>

<p>====== SADD ======
1000000 requests completed in 10.74 seconds
50 parallel clients
3 bytes payload
keep alive: 1</p>

<p>99.88% &lt;= 1 milliseconds
99.95% &lt;= 2 milliseconds
99.97% &lt;= 3 milliseconds
99.99% &lt;= 4 milliseconds
100.00% &lt;= 4 milliseconds
93144.56 requests per second
```</p>

<p>如作者所言, 性能几乎可以跟直连redis比拟，背后的数据也很均匀,使用twemproxy 观察连接数, 一直都保持在个位数左右。</p>

<h3 id="第五节">摘自极光博客的评论</h3>

<ul>
  <li>前端使用 Twemproxy 做代理，后端的 Redis 数据能基本上根据 key 来进行比较均衡的分布。</li>
  <li>后端一台 Redis 挂掉后，Twemproxy 能够自动摘除。恢复后，Twemproxy 能够自动识别、恢复并重新加入到 Redis 组中重新使用。</li>
  <li>Redis 挂掉后，后端数据是否丢失依据 Redis 本身的策略配置，与 Twemproxy 基本无关。</li>
  <li>如果要新增加一台 Redis，Twemproxy 需要重启才能生效；并且数据不会自动重新 Reblance，需要人工单独写脚本来实现。</li>
  <li>如同时部署多个 Twemproxy，配置文件一致（测试配置为distribution：ketama,modula），则可以从任意一个读取，都可以正确读取 key对应的值。</li>
  <li>多台 Twemproxy 配置一样，客户端分别连接多台 Twemproxy可以在一定条件下提高性能。根据 Server 数量，提高比例在 110-150%之间。</li>
  <li>如原来已经有 2 个节点 Redis，后续有增加 2 个 Redis，则数据分布计算与原来的 Redis 分布无关，现有数据如果需要分布均匀的话，需要人工单独处理。</li>
  <li>如果 Twemproxy 的后端节点数量发生变化，Twemproxy 相同算法的前提下，原来的数据必须重新处理分布，否则会存在找不到key值的情况。</li>
</ul>

<hr />

<p>参考链接</p>

<p><a href="http://blog.jpush.cn/redis-twemproxy-benchmark/">极光推送的博客</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为服务端程序构建 Docker]]></title>
    <link href="http://zheng-ji.github.com/blog/2015/04/05/yong-bao-docker/"/>
    <updated>2015-04-05T20:24:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2015/04/05/yong-bao-docker</id>
    <content type="html"><![CDATA[<p>Docker 的优点自从问世就一直被工业界热论。</p>

<p>平时工作中，所部署的大多数<code>Python</code>项目都会用上 <a href="http://wiki.zheng-ji.info/Python/virtualenv-py.html">virtualenv</a>, 
沙箱隔离带来的好处不言而喻。我也希望静态编译的服务，比如 <code>Golang</code> <code>C++</code> 的项目
同样能使用上沙箱环境。得益于<code>Docker</code>，我们仍然可以做到。</p>

<p>这个过程没有想象中的简单，需要一番折腾，我以最近写的 KafkServer 为例，叙述我是怎么构建的，需要读者具备一定的 Docker 基础. 或许这不是最好的方法。</p>

<h3 id="docker-">一览该 Docker 项目</h3>

<p><code>
zj@zheng-ji:~/workspace/gocode/src/kafconsumer/docker$ tree
.
├── Dockerfile
├── kafConsumer
│   ├── consumer
│   ├── etc
│   │   ├── config.yml
│   │   └── logger.xml
│   └── script
│       └── start.sh
└── kafConsumer.tar.gz
</code></p>

<p>以上的截图，是一个完整的 <code>Docker</code> 项目，包含了：</p>

<ul>
  <li><code>Dockerfile</code>,</li>
  <li><code>kafCounsumer</code>(服务端程序，里面附带的启动脚本，配置程序，以及二进制文件)，</li>
  <li>还有它被压缩而成的 <code>kafConsumer.tar.gz</code></li>
</ul>

<hr />

<h3 id="dockerfile-">Dockerfile 的内容</h3>

<p><code>
FROM ubuntu:14.04                                                         
MAINTAINER zheng-ji &lt;zheng-ji.info&gt;                                     
RUN echo Asia/Shanghai &gt; /etc/timezone                   
RUN sed -i "s/archive\.ubuntu/mirrors.163/" /etc/apt/sources.list          
RUN apt-get update                                                         
COPY kafConsumer.tar.gz /                                                  
RUN tar xvf kafConsumer.tar.gz                                         
VOLUME /data                   
WORKDIR /kafConsumer                                                   
ENTRYPOINT ["./script/start.sh"]
</code></p>

<p><code>Dockerfile</code> 可以理解为<code>makefile</code> 之类的文件，Docker 可以依照文件中的内容，构建镜像.</p>

<p><code>
sudo docker -t build Server/KafConsumer .
</code></p>

<p>这样就生成了<code>Tag</code> 为 <code>Server/KafConsumer</code> 的镜像，待会儿我们会使用它</p>

<p>以上 <code>Dockerfile</code> 的具体内容的意义是:</p>

<blockquote>

  <ul>
    <li>第一行：拉取ubuntu 14:04的镜像源</li>
    <li>第二行：维护者</li>
    <li>第三行：调整时区</li>
    <li>第四行：更新源地址</li>
    <li>第五行：更新源</li>
    <li>第六行：复制项目下的压缩包到虚拟机根目录</li>
    <li>第七行：解压</li>
    <li>第八行：项目中使用/data数据卷</li>
    <li>第九行：进入工作目录</li>
    <li>第十行：Docker的入口执行文件是start.sh</li>
  </ul>
</blockquote>

<hr />

<h3 id="section">入口文件的内容</h3>

<p><code>
#!/bin/bash
ulimit -a
if [ ! -d /data/ad ];  then
    mkdir /data/ad
fi
exec ./consumer -c=etc/config.yml
</code></p>

<p>这是一个shell的启动文件，因此一定要在开头写明 #!/bin/bash, 使用exec 执行程序</p>

<hr />

<h3 id="section-1">启动镜像</h3>

<p><code>
sudo docker run -i -t  -v /path/to/data:/data Server/kafConsumer
</code>
这样就执行了，-v 可以映射你的本地文件到虚拟机的某个数据卷，这样我们就能从外面看到程序产生的文件.</p>

<h3 id="section-2">如果你想关闭或者重启该服务的怎么办</h3>

<p>```
sudo docker ps -a</p>

<p>找到你的 Docker 容器</p>

<p>CONTAINER ID    IMAGE           COMMAND                CREATED        STATUS        PORTS    NAMES
5b39d0d5cb85    Server/kafkaconsumer:latest   “./script/start.sh”    3 hours ago    tender_bohr 
```</p>

<p>启动或者关闭</p>

<p><code>
sudo docker start tender_bohr
sudo docker stop tender_bohr
</code></p>

<hr />

<h3 id="daocloud--">Daocloud  加速</h3>

<p>功夫墙的原因，国外很多镜像被墙，因此构建镜像很慢，使用 Daocloud 服务可以加速,注册后就有该服务了</p>

<p><code>
cat /etc/default/docker
DOCKER_OPTS="$DOCKER_OPTS --registry-mirror=http://xxxxxx.m.daocloud.io"
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[rsyslog 接收远程日志]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/07/27/rsyslog-jie-shou-yuan-cheng-ri-zhi/"/>
    <updated>2014-07-27T14:26:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/07/27/rsyslog-jie-shou-yuan-cheng-ri-zhi</id>
    <content type="html"><![CDATA[<p>Rsyslog 接收远程日志</p>

<p>需要开启运程模式, 以ubuntu为例子</p>

<p><code>
vim /etc/default/rsyslog
RSYSLOGD_OPTIONS="-c5 -r -x"
</code></p>

<p>编写模板,文档中说到要在<code>rsyslog.conf</code>里面编辑</p>

<p><code>
vim /etc/rsyslog.conf
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat
$template DynFile, "/data/log/%$year%%$month%%$day%/%$year%%$month%%$day%%$hour%.log"
$template dotalogformat, "%msg%\n"
</code></p>

<p>编写过滤规则, 修改 <code>/etc/rsysconf.d/your_business.conf</code></p>

<p>```
# 开通端口
$ModLoad imtcp
$InputTCPServerRun 1514</p>

<h1 id="section">过滤规则</h1>
<p>if $msg contains “xx” then ?DynFile;dotalogformat</p>

<h1 id="sysloglog-">为了不让它写入syslog.log 而直接写入目标模板</h1>
<p>:msg, contains, “xx” ~
```</p>

<p>重启服务</p>

<p><code>
sudo service rsyslog start
</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[折腾 Docker]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/07/16/zhe-teng-docker/"/>
    <updated>2014-07-16T20:20:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/07/16/zhe-teng-docker</id>
    <content type="html"><![CDATA[<p>早在1年前就听过 <a href="http://docs.docker.com/">docker</a>这个 Golang 社区的明星产品 。</p>

<h3 id="section">简单地介绍</h3>

<p>Docker 提供了一个可以运行你的应用程序的容器。像一个可移植的容器引擎那样工作。它把应用程序及所有程序的依赖环境打包到一个虚拟容器中，这个虚拟容器可以运行在任何一种 Linux 服务器上。这大大地提高了程序运行的灵活性和可移植性，极大的降低运维成本。</p>

<h3 id="section-1">组成</h3>

<ul>
  <li>Docker 服务器守护程序（server daemon），用于管理所有的容器。</li>
  <li>Docker 命令行客户端，用于控制服务器守护程序。</li>
  <li>Docker 镜像：查找和浏览 docker 容器镜像。它也访问这里得到：<a href="https://index.docker.io/">链接</a></li>
</ul>

<h3 id="docker">有了虚拟机为什么还要docker?</h3>
<p>virtualbox 等虚拟机提供的是完整的操作系统环境, 迁移的时候太大了。它们包含了大量类似硬件驱动、虚拟处理器、网络接口等等并不需要的信息，也需要比较长时间的启动，同时也会消耗大量的内存、CPU 资源。</p>

<p>Docker 相比起来就非常轻量级了。运行起来就和一个常规程序差不多。这个容器不仅仅运行快，创建一个镜像和制作文件系统快照也很快,甚至比vagrant更节约资源</p>

<h3 id="section-2">初体验</h3>
<p>下载docker 并安装 ubuntu 12.04 这里如果没有指明 <code>ubuntu:12.04</code>, 会将所有ubuntu镜像都下载。由于被墙，速度惨不忍睹.</p>

<p><code>
zj@zheng-ji:~$ sudo apt-get install docker.io
zj@zheng-ji:~$ sudo docker pull ubuntu:12.04 
</code></p>

<p>查看已有的镜像</p>

<p><code>
zj@zheng-ji:~$ sudo docker images
</code></p>

<p>使用 bash 命令进入docker的指定镜像</p>

<p><code>
zj@zheng-ji:~$ sudo docker run -t -i -p 3000 ubuntu:12.04 /bin/bash
</code></p>

<p>这时候我就可以像一个新系统一样把玩了, 比如创建一个目录。</p>

<p><code>
zj@zheng-ji:~$ mkdir /home/zj/test
</code></p>

<p>好了，我想把这个现场保存下来做移植到别的地方直接使用。需要到 <a href="https://registry.hub.docker.com/u/">这里</a> 注册一个账户，然后再上传，类似在 github  上面一样的操作。</p>

<p>```
sudo docker ps </p>

<p>//因为我已经commit 过一次，所以名字变成我的别名 zhengji/helloworld
zj@zheng-ji:~$ docker ps
CONTAINER ID        IMAGE                       COMMAND             CREATED             STATUS              PORTS                     NAMES
8dad3aa4451f        zhengji/helloworld:latest   bin/bash            27 minutes ago      Up 27 minutes       0.0.0.0:49156-&gt;3000/tcp   high_galileo    <br />
```</p>

<p>可以看到要保存的镜像的 CONTAINER ID</p>

<p>这个时候提交</p>

<p>```
zj@zheng-ji:~$ sudo docker commit 8dad3aa4451f zhengji/helloworld -m “test”
61f9527f368395ee228af07062b3f1a26fa7143ba2721c4f9755ae93d588358e</p>

<p>zj@zheng-ji:~$ sudo docker push zhengji/helloworld
```</p>

<p>下次pull 下来就可以使用了</p>

<p><code>
sudo docker pull zhengji/helloworld
</code></p>

<hr />

<h3 id="section-3">可能遇到的坑</h3>

<ul>
  <li>需要编辑 /etc/default/docker.io 然后编辑里面的,使得其可以解决 DNS解析</li>
</ul>

<p><code>
DOCKER_OPTS = "-dns 8.8.8.8"
</code></p>

<ul>
  <li>设置 ufw 端口可转发</li>
</ul>

<p><code>
vim /etc/default/ufw
DEFAULT_FORWARD_POLICY = "ACCEPT"
</code></p>

<h3 id="section-4">参考链接</h3>

<ul>
  <li><a href="http://www.docker.org.cn/book/docker.html">docker 中文</a></li>
  <li><a href="http://www.docker.org.cn/book/docker.html">docker官网</a></li>
  <li><a href="http://segmentfault.com/a/1190000000366923">Docker入门教程</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
