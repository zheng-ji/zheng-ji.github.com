<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: System | 织网]]></title>
  <link href="http://zheng-ji.github.com/blog/categories/system/atom.xml" rel="self"/>
  <link href="http://zheng-ji.github.com/"/>
  <updated>2014-11-25T22:57:53+08:00</updated>
  <id>http://zheng-ji.github.com/</id>
  <author>
    <name><![CDATA[zheng-ji]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Nginx与php-fpm系统参数配置]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/11/02/nginxyu-phpxi-tong-can-shu-pei-zhi/"/>
    <updated>2014-11-02T15:46:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/11/02/nginxyu-phpxi-tong-can-shu-pei-zhi</id>
    <content type="html"><![CDATA[<p>需要为机器调配参数了, 早些时候看过不少文章, 但没经历过始终不深刻, 以下讲的是在 8core 8G，Centos 配置 php-fpm 与 NGINX。</p>

<h3 id="php-fpm-">php-fpm 参数配置</h3>

<p>关于 php-fpm 的配置,Ubuntu系统中 <code>/etc/php5/fpm/pool.d</code>目录下, Centos是<code>/etc/php-fpm.d</code>目录下编辑<code>www.conf</code></p>

<p><code>
listen = /var/run/php5-fpm.sock
listen.backlog = -1 (on FreeBSD -1 unlimit)
pm = static # 使用静态进程管理
pm.max_children = 128
request_terminate_timeout = 8s #设置太大，会导致work进程过多，来不及kill掉
</code></p>

<p>编辑 <code>/etc/default/php5-fpm</code></p>

<p><code>
ulimit -n 655360
</code></p>

<h3 id="nginx-">Nginx 配合参数</h3>

<ul>
  <li>启用irqbalance</li>
</ul>

<p>由于精简系统的服务没有开启irqbalance，irqbalance现在被证实为非常有必要的服务，他的主要功能是可以合理的调配使用各个 CPU 核心，特别是对于目前主流多核心的 CPU，简单的说就是能够把压力均匀的分配到各个 CPU 核心上，对提升性能有很大的帮助。</p>

<p><code>
shell&gt; yum -y install irqbalance
shell&gt; service irqbalance start
cat /proc/irqbalance #查看中断的分布
</code></p>

<ul>
  <li>为nginx 绑定 cpu</li>
</ul>

<p><code>
worker_rlimit_nofile 300000;
worker_processes  8;
worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000;
</code></p>

<ul>
  <li>连接数调整，</li>
</ul>

<p>nginx发起的连接数，远远超过了 php-fpm 所能处理的数目，导致端口（或socket）频繁被锁，造成堵塞。</p>

<p>```
vi /etc/sysctl.conf 进行了微调
fs.file-max = 6553600</p>

<p>vim /etc/security/limits.conf
* soft nofile 655360
* hard nofile 655360</p>

<p>vim /etc/nginx/nginx.conf</p>

<p>worker_rlimit_nofile 300000;
events {
    worker_connections 300000;
    use epoll;
}
http {
    keepalive_timeout  0; #关闭keepalive_timeout, 快速释放系统资源
}
```</p>

<p>从查到的资料看起来 8 核 理論值的最大連線數 = <code>worker_processes * worker_connections / 8</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iptable做NAT转发]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/29/iptablezuo-natzhuan-fa/"/>
    <updated>2014-10-29T22:38:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/29/iptablezuo-natzhuan-fa</id>
    <content type="html"><![CDATA[<p>简单简述下我遇到的问题:</p>

<p>现在局域网有2台机器, 其中一台机器(下文我们称之为ServerA)可以访问外网,有独立IP,而另外一台机器(ServerB)访问不了外网, 需要想办法让 ServerB 也能上网。</p>

<ul>
  <li>ServerA 操作</li>
</ul>

<p><code>
vi /etc/sysctl.conf
net.ipv4.ip_forward = 1
iptables -t nat -A POSTROUTING -s 10.4.0.0/16 -j MASQUERADE
</code></p>

<p>ServerB</p>

<p>编辑 /etc/network/interface
主要是修改gateway 参数，指向 ServerA 的 IP</p>

<p><code>
sudo ifdown eth0; sudo ifup eth0</code> 
ServerB 就可以连接外网了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[内存都去哪儿啦]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/13/nei-cun-du-qu-na-er-la/"/>
    <updated>2014-10-13T21:20:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/13/nei-cun-du-qu-na-er-la</id>
    <content type="html"><![CDATA[<h3 id="section">看到的现象</h3>

<p>今天在游戏服务器上使用了top 命令系统使用参数，发现内存16G 的内存用了12G, 如下
<img src="/images/2014/10/top.png">
但是同时在线人数不多，机器的负载并不是很大，并发也不高, 只有900+。
<img src="/images/2014/10/nestat.png">
而游戏的服务端代码仅仅使用了7.8G, 我们不禁会问，内存去哪里了？难道是系统自己占用的内存呢，不可能一个8核16G的机器系统自己占用了4G吧?</p>

<p>带着这个疑问，查阅了资料，发现其中并非表明所看到的这么简单. 我们来看以下 使用 <code>free -m</code> 命令
<img src="/images/2014/10/free.png"></p>

<h3 id="mem--">Mem 参数 解释</h3>
<ul>
  <li>total 内存总数: 15875 , total = used + free</li>
  <li>used 已经使用的内存数: 11900</li>
  <li>free 空闲的内存数: 3975</li>
  <li>shared 当前已经废弃不用，总是0</li>
  <li>buffers: Buffer Cache内存数: 145</li>
  <li>cached: Page Cache内存数: 5561</li>
</ul>

<h3 id="bufferscache">-/+ buffers/cache的解惑</h3>
<ul>
  <li>-buffers/cache 的内存数: 6193 (大致等于第1行的 used - buffers - cached), 反映的是被程序实实在在吃掉的内存，</li>
  <li>+buffers/cache 的内存数: 9682 (大致等于第1行的 free + buffers + cached), 反映的是可以挪用的内存总数</li>
</ul>

<h3 id="cache">两种cache</h3>
<p>因为cpu速度明显快过内存， 为了提高磁盘存取效率, Linux做了一些精心的设计, 采取了两种主要Cache方式, 来做速度的过度, 这些Cache有效缩短了 I/O系统调用(如read,write,getdents)的时间。</p>

<ul>
  <li>Buffer Cache, 针对磁盘块的读写</li>
  <li>Page Cache。针对文件inode的读写。</li>
</ul>

<h3 id="section-1">内存解读的区别</h3>

<p>第1行<code>(mem)的used/free</code>与第2行<code>(-/+ buffers/cache) used/free</code>的区别在于角度的不同:</p>

<ul>
  <li>第一行因为对于OS，buffers/cached 都是属于被使用，所以他的可用内存是3975M,已用内存是11900MB,其中包括,内核（OS）使用 + 应用使用的+ buffers + cached.</li>
  <li>第2行所指的是从应用程序角度来看，对于应用程序来说，buffers/cached 是可用的，是为了提高文件读取的性能而设，当应用程序要用到内存的时候，buffer/cached会很快地被回收。所以从应用程序的角度来说，</li>
</ul>

<p><code>
可用内存=系统free memory + buffers + cached.
</code>
所以，回到话题开头，虽然内存显示用了12.2G，正在被应用程序使用的是7.9G 还有5.6(cache+buffer) +  4G free 可用.内存原来在这里！</p>

<p>获益匪浅 :) </p>
]]></content>
  </entry>
  
</feed>
