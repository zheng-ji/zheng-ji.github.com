<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: System | 织网]]></title>
  <link href="http://zheng-ji.github.com/blog/categories/system/atom.xml" rel="self"/>
  <link href="http://zheng-ji.github.com/"/>
  <updated>2015-04-06T18:59:57+08:00</updated>
  <id>http://zheng-ji.github.com/</id>
  <author>
    <name><![CDATA[zheng-ji]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[nginx错误码]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/12/13/nginxcuo-wu-ma/"/>
    <updated>2014-12-13T15:18:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/12/13/nginxcuo-wu-ma</id>
    <content type="html"><![CDATA[<p>在定位线上服务问题的时候，通常会去查看<code>Nginx</code> 的<code>error log</code></p>

<p>那么 error 的定义, 对查找问题就显得很有帮助</p>

<ul>
  <li>upstream prematurely closed connection</li>
</ul>

<blockquote>
  <p>请求uri的时候出现的异常，是由于 upstream 还未返回应答给用户时用户断掉连接造成的，对系统没有影响，可以忽略</p>
</blockquote>

<ul>
  <li>recv() failed (104: Connection reset by peer) </li>
</ul>

<blockquote>
  <p>服务器的并发连接数超过了其承载量，服务器会将其中一些连接Down掉；客户关掉了浏览器，而服务器还在给客户端发送数据;</p>
</blockquote>

<ul>
  <li>(111: Connection refused) while connecting to upstream </li>
</ul>

<blockquote>
  <p>用户在连接时，若遇到后端 upstream 挂掉或者不通，会收到该错误</p>
</blockquote>

<ul>
  <li>(111: Connection refused) while reading response header from upstream </li>
</ul>

<blockquote>
  <p>用户在连接成功后读取数据时，若遇到后端 upstream 挂掉或者不通，会收到该错误</p>
</blockquote>

<ul>
  <li>(111: Connection refused) while sending request to upstream </li>
</ul>

<blockquote>
  <p>Nginx 和 upstream 连接成功后发送数据时，若遇到后端 upstream 挂掉或者不通，会收到该错误</p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while connecting to upstream </li>
</ul>

<blockquote>
  <p>nginx 连接后面的 upstream 时超时</p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while reading upstream </li>
</ul>

<blockquote>
  <p>nginx 读取来自 upstream 的响应时超时 </p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while reading response header from upstream </li>
</ul>

<blockquote>
  <p>nginx 读取来自 upstream 的响应头时超时</p>
</blockquote>

<ul>
  <li>(110: Connection timed out) while reading upstream </li>
</ul>

<blockquote>
  <p>nginx读取来自 upstream 的响应时超时</p>
</blockquote>

<ul>
  <li>(104: Connection reset by peer) while connecting to upstream </li>
</ul>

<blockquote>
  <p>upstream发送了 RST，将连接重置</p>
</blockquote>

<ul>
  <li>upstream sent invalid header while reading response header from upstream </li>
</ul>

<blockquote>
  <p>upstream 发送的响应头无效</p>
</blockquote>

<ul>
  <li>upstream sent no valid HTTP/1.0 header while reading response header from upstream</li>
</ul>

<blockquote>
  <p>upstream 发送的响应头无效</p>
</blockquote>

<ul>
  <li>client intended to send too large body </li>
</ul>

<blockquote>
  <p>用于设置允许接受的客户端请求内容的最大值，默认值是1M，client 发送的 body 超过了设置值</p>
</blockquote>

<ul>
  <li>reopening logs </li>
</ul>

<blockquote>
  <p>用户发送kill  -USR1命令</p>
</blockquote>

<ul>
  <li>gracefully shutting down</li>
</ul>

<blockquote>
  <p>用户发送kill  -WINCH命令</p>
</blockquote>

<ul>
  <li>no live upstreams while connecting to upstream </li>
</ul>

<blockquote>
  <p>upstream 下的 server 全都挂了</p>
</blockquote>

<ul>
  <li>SSL_do_handshake() failed</li>
</ul>

<blockquote>
  <p>SSL握手失败</p>
</blockquote>

<ul>
  <li>ngx_slab_alloc() failed: no memory in SSL session shared cache</li>
</ul>

<blockquote>
  <p>ssl_session_cache大小不够等原因造成</p>
</blockquote>

<ul>
  <li>could not add new SSL session to the session cache while SSL handshaking</li>
</ul>

<blockquote>
  <p>ssl_session_cache 大小不够等原因造成</p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用到的Tcpdump]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/11/29/yong-dao-de-tcpdump/"/>
    <updated>2014-11-29T14:15:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/11/29/yong-dao-de-tcpdump</id>
    <content type="html"><![CDATA[<p>开发中，要定位具体问题，特别是网络问题的时候，多数是要晋出<code>tcpdump</code>，遗憾的是我略懂皮毛，有必要深入一些。
简单说下我常用的 TcpDump的方法</p>

<p><code>
tcpdump -i eth0 -Xxn port 80 -s 0 -c 1024 
</code></p>

<p>如果仅仅是看manual  多数时候还是会忘记，好记性不如烂笔头，上述的选项是我认为很有用的</p>

<p><code>
-i 指定网卡
-Xxn X使用Ascii和16进制，n 表示 ip 用数字表示
-s 0 表示整包抓取
-c 1024 表示包得大小
</code></p>

<p>如果希望将抓包过程中保留下来，可以在上述命令尾部加上 <code>-w trace.cap</code>
这种格式的文件，文本编辑器是无法理解，需要特殊的软件才能回复，比如 <code>wireshark</code></p>

<p>Tcpdump 中的 flag 有必要提下：</p>

<ul>
  <li>PSH 代表要求发送立即发送缓冲区内的其他对应数据包，无需缓冲区满才发送</li>
  <li>RST 如果RST=1表示连接马上结束，无需等待终止确认手续，发送端已经断线</li>
  <li>SYNC 表示主动连接到对方，建立连接</li>
  <li>FIN 表示传送结束，发送方等待对方响应</li>
</ul>

<p>通过 wireshark 可以再现所谓的三次握手和四次挥手过程。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx与php-fpm系统参数配置]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/11/02/nginxyu-phpxi-tong-can-shu-pei-zhi/"/>
    <updated>2014-11-02T15:46:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/11/02/nginxyu-phpxi-tong-can-shu-pei-zhi</id>
    <content type="html"><![CDATA[<p>需要为机器调配参数了, 早些时候看过不少文章, 但没经历过始终不深刻, 以下讲的是在 8core 8G，Centos 配置 php-fpm 与 NGINX。</p>

<h3 id="php-fpm-">php-fpm 参数配置</h3>

<p>关于 php-fpm 的配置,Ubuntu系统中 <code>/etc/php5/fpm/pool.d</code>目录下, Centos是<code>/etc/php-fpm.d</code>目录下编辑<code>www.conf</code></p>

<p><code>
listen = /var/run/php5-fpm.sock
listen.backlog = -1 (on FreeBSD -1 unlimit)
pm = static # 使用静态进程管理
pm.max_children = 128
request_terminate_timeout = 8s #设置太大，会导致work进程过多，来不及kill掉
</code></p>

<p>编辑 <code>/etc/default/php5-fpm</code></p>

<p><code>
ulimit -n 655360
</code></p>

<h3 id="nginx-">Nginx 配合参数</h3>

<ul>
  <li>启用irqbalance</li>
</ul>

<p>由于精简系统的服务没有开启irqbalance，irqbalance现在被证实为非常有必要的服务，他的主要功能是可以合理的调配使用各个 CPU 核心，特别是对于目前主流多核心的 CPU，简单的说就是能够把压力均匀的分配到各个 CPU 核心上，对提升性能有很大的帮助。</p>

<p><code>
shell&gt; yum -y install irqbalance
shell&gt; service irqbalance start
cat /proc/irqbalance #查看中断的分布
</code></p>

<ul>
  <li>为nginx 绑定 cpu</li>
</ul>

<p><code>
worker_rlimit_nofile 300000;
worker_processes  8;
worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000;
</code></p>

<ul>
  <li>连接数调整，</li>
</ul>

<p>nginx发起的连接数，远远超过了 php-fpm 所能处理的数目，导致端口（或socket）频繁被锁，造成堵塞。</p>

<p>```
vi /etc/sysctl.conf 进行了微调
fs.file-max = 6553600</p>

<p>vim /etc/security/limits.conf
* soft nofile 655360
* hard nofile 655360</p>

<p>vim /etc/nginx/nginx.conf</p>

<p>worker_rlimit_nofile 300000;
events {
    worker_connections 300000;
    use epoll;
}
http {
    keepalive_timeout  0; #关闭keepalive_timeout, 快速释放系统资源
}
```</p>

<p>从查到的资料看起来 8 核 理論值的最大連線數 = <code>worker_processes * worker_connections / 8</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iptable做NAT转发]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/29/iptablezuo-natzhuan-fa/"/>
    <updated>2014-10-29T22:38:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/29/iptablezuo-natzhuan-fa</id>
    <content type="html"><![CDATA[<p>简单简述下我遇到的问题:</p>

<p>现在局域网有2台机器, 其中一台机器(下文我们称之为ServerA)可以访问外网,有独立IP,而另外一台机器(ServerB)访问不了外网, 需要想办法让 ServerB 也能上网。</p>

<ul>
  <li>ServerA 操作</li>
</ul>

<p><code>
vi /etc/sysctl.conf
net.ipv4.ip_forward = 1
iptables -t nat -A POSTROUTING -s 10.4.0.0/16 -j MASQUERADE
</code></p>

<p>ServerB</p>

<p>编辑 /etc/network/interface
主要是修改gateway 参数，指向 ServerA 的 IP</p>

<p><code>
sudo ifdown eth0; sudo ifup eth0</code> 
ServerB 就可以连接外网了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[内存都去哪儿啦]]></title>
    <link href="http://zheng-ji.github.com/blog/2014/10/13/nei-cun-du-qu-na-er-la/"/>
    <updated>2014-10-13T21:20:00+08:00</updated>
    <id>http://zheng-ji.github.com/blog/2014/10/13/nei-cun-du-qu-na-er-la</id>
    <content type="html"><![CDATA[<h3 id="section">看到的现象</h3>

<p>今天在游戏服务器上使用了top 命令系统使用参数，发现内存16G 的内存用了12G, 如下
<img src="/images/2014/10/top.png">
但是同时在线人数不多，机器的负载并不是很大，并发也不高, 只有900+。
<img src="/images/2014/10/nestat.png">
而游戏的服务端代码仅仅使用了7.8G, 我们不禁会问，内存去哪里了？难道是系统自己占用的内存呢，不可能一个8核16G的机器系统自己占用了4G吧?</p>

<p>带着这个疑问，查阅了资料，发现其中并非表明所看到的这么简单. 我们来看以下 使用 <code>free -m</code> 命令
<img src="/images/2014/10/free.png"></p>

<h3 id="mem--">Mem 参数 解释</h3>
<ul>
  <li>total 内存总数: 15875 , total = used + free</li>
  <li>used 已经使用的内存数: 11900</li>
  <li>free 空闲的内存数: 3975</li>
  <li>shared 当前已经废弃不用，总是0</li>
  <li>buffers: Buffer Cache内存数: 145</li>
  <li>cached: Page Cache内存数: 5561</li>
</ul>

<h3 id="bufferscache">-/+ buffers/cache的解惑</h3>
<ul>
  <li>-buffers/cache 的内存数: 6193 (大致等于第1行的 used - buffers - cached), 反映的是被程序实实在在吃掉的内存，</li>
  <li>+buffers/cache 的内存数: 9682 (大致等于第1行的 free + buffers + cached), 反映的是可以挪用的内存总数</li>
</ul>

<h3 id="cache">两种cache</h3>
<p>因为cpu速度明显快过内存， 为了提高磁盘存取效率, Linux做了一些精心的设计, 采取了两种主要Cache方式, 来做速度的过度, 这些Cache有效缩短了 I/O系统调用(如read,write,getdents)的时间。</p>

<ul>
  <li>Buffer Cache, 针对磁盘块的读写</li>
  <li>Page Cache。针对文件inode的读写。</li>
</ul>

<h3 id="section-1">内存解读的区别</h3>

<p>第1行<code>(mem)的used/free</code>与第2行<code>(-/+ buffers/cache) used/free</code>的区别在于角度的不同:</p>

<ul>
  <li>第一行因为对于OS，buffers/cached 都是属于被使用，所以他的可用内存是3975M,已用内存是11900MB,其中包括,内核（OS）使用 + 应用使用的+ buffers + cached.</li>
  <li>第2行所指的是从应用程序角度来看，对于应用程序来说，buffers/cached 是可用的，是为了提高文件读取的性能而设，当应用程序要用到内存的时候，buffer/cached会很快地被回收。所以从应用程序的角度来说，</li>
</ul>

<p><code>
可用内存=系统free memory + buffers + cached.
</code>
所以，回到话题开头，虽然内存显示用了12.2G，正在被应用程序使用的是7.9G 还有5.6(cache+buffer) +  4G free 可用.内存原来在这里！</p>

<p>获益匪浅 :) </p>
]]></content>
  </entry>
  
</feed>
